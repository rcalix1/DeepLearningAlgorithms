{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install tiktoken torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e992d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken # For tokenization import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F import numpy as np\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Initialize Tiktoken tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\") # Using GPT-2 encoding for tokenization\n",
    "# Hyperparameters\n",
    "block_size = 40 # Number of tokens in each sequence batch_size = 64\n",
    "max_iters = 6000\n",
    "eval_interval = 500\n",
    "learning_rate = 0.0003\n",
    "eval_iters = 300\n",
    "n_embd = 512 # Embedding dimension\n",
    "n_head = 8 # Number of attention heads\n",
    "n_layer = 6 # Number of transformer layers\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load text data\n",
    "input_file = 'Final_dataset2.txt'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# Encode data with Tiktoken\n",
    "encoded_data = enc.encode(text)\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "# Split data into training and validation sets\n",
    "n = int(0.9 * len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe60683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4efb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch(split):\n",
    "data = train_data if split == \"train\" else val_data\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,)) x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "y = torch.stack([data[i+1:i+1+block_size] for i in ix]) x, y = x.to(device), y.to(device)\n",
    "return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "def __init__(self, head_size):\n",
    "super().__init__()\n",
    "self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "self.tril = torch.tril(torch.ones(block_size, block_size)).to(device) self.dropout = nn.Dropout(dropout)\n",
    "def forward(self, x): k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:wei.size(1), :wei.size(1)] == 0,␣\n",
    "↪float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) wei = self.dropout(wei)\n",
    "v = self.value(x)\n",
    "return wei @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caadd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module): def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    ")\n",
    "        \n",
    "def forward(self, x):\n",
    "return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) self.proj = nn.Linear(n_embd, n_embd)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "def forward(self, x):\n",
    "out = torch.cat([h(x) for h in self.heads], dim=-1) out = self.dropout(self.proj(out))\n",
    "return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "def forward(self, x):\n",
    "x = x + self.sa(self.ln1(x))\n",
    "x = x + self.ffwd(self.ln2(x)) return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08840fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTModel(nn.Module): def __init__(self):\n",
    "        super().__init__()\n",
    "self.token_embedding_table = nn.Embedding(enc.n_vocab, n_embd) # Use␣ ↪enc.n_vocab here\n",
    "        self.pos_emb_table = nn.Embedding(block_size, n_embd)\n",
    "self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in␣ ↪range(n_layer)])\n",
    "self.ln_f = nn.LayerNorm(n_embd)\n",
    "self.lm_head = nn.Linear(n_embd, enc.n_vocab) # Use enc.n_vocab here\n",
    "\n",
    "def forward(self, idx, targets=None):\n",
    "tok_emb = self.token_embedding_table(idx)\n",
    "pos_emb = self.pos_emb_table(torch.arange(idx.size(1), device=device)) x = tok_emb + pos_emb\n",
    "x = self.blocks(x)\n",
    "x = self.ln_f(x)\n",
    "logits = self.lm_head(x)\n",
    "if targets is None: return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets) return logits, loss\n",
    "    \n",
    "def generate(self, idx, max_new_tokens): for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, _ = self(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GPTModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Function to estimate training and validation loss\n",
    "\n",
    "\n",
    "def estimate_loss(): model.eval()\n",
    "out = {}\n",
    "for split in ['train', 'val']:\n",
    "losses = torch.zeros(eval_iters) for k in range(eval_iters):\n",
    "X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "model.train() return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "  if iter % eval_interval == 0:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss␣ ↪{losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate text starting from an initial token\n",
    "start_token = torch.ones((1, 1), dtype=torch.long, device=device) # Adjust␣\n",
    "↪starting token if needed\n",
    "generated_tokens = model.generate(start_token, max_new_tokens=500) generated_text = enc.decode(generated_tokens[0].tolist()) print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf76845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "# Function to ask a question and include both the question and answer in output\n",
    "def ask_question(model, question, max_new_tokens=500):\n",
    "# Encode the question\n",
    "question_tokens = enc.encode(question)\n",
    "input_ids = torch.tensor([question_tokens], dtype=torch.long, device=device)\n",
    "    # Generate answer with controlled length\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "max_new_tokens=max_new_tokens # Limits the number of tokens generated␣ ↪after the input\n",
    ")\n",
    "    # Convert generated tokens to list and decode the full sequence\n",
    "    generated_tokens = generated_ids[0].tolist()\n",
    "full_output = enc.decode(generated_tokens) # Decodes the full question and␣ ↪generated answer\n",
    "return full_output.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76300e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to prepare and generate answer (alternative function)\n",
    "def prepare_and_generate(model, question, max_new_tokens=300):\n",
    "# Encode the question as tokens\n",
    "encoded_question = enc.encode(question)\n",
    "context_tensor = torch.tensor(encoded_question, dtype=torch.long,␣\n",
    "↪device=device).view(1, -1)\n",
    "    # Generate the answer based on the question as context\n",
    "    generated_ids = model.generate(\n",
    "        context_tensor,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    # Decode the full sequence, including the question and the answer\n",
    "full_output = enc.decode(generated_ids[0].tolist()) # Decodes both␣ ↪question and generated answer\n",
    "return full_output.strip()\n",
    "# Example usage\n",
    "question = \"When is the ideal age to begin potty training my bunny, and how␣ ↪long will it take to be successful?\"\n",
    "answer = ask_question(model, question) # Using ask_question function print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efea5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63914a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880adef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ab7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b31b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c801209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc94a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43934b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
