<html>
<head>

  <link href="style.css" rel="stylesheet" type="text/css" />
</head>

  <body>

<div class="navbar">
  <a href="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/index.html"> Deep Learning </a>
  <a href="https://ricardocalix.substack.com">Substack</a>
  <a href="https://www.youtube.com/channel/UCKRgi-HJDEq0a3nhlG2nQvg">YouTube</a>
  <a href="https://github.com/rcalix1/DeepLearningAlgorithms/tree/main/SecondEdition">GitHub</a>
  <a href="https://www.galacticbackwater.com/theAIhub/index.html">Recommender</a>
  <a href="https://amzn.to/3OauEG0">Books</a>
  <a href="https://www.linkedin.com/in/ricardo-calix-phd">About</a>
  <a href="https://scholar.google.com/citations?hl=en&user=TiKVs6AAAAAJ">Scholar</a>	
  <a href="">Shop</a>
  <a href="https://www.rcalix.com">Contact</a>
</div>

    

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->

<div class="main">    <!-- for the fixed nav bar -->

<h1>Chapter  - Generative Adversarial Networks (GANs)</h1>

    <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/ganNN.300dpi.jpg" height="400" width="auto">
      </div>

    </center>

<p>
	In this section of the book I will cover Generative Adversarial Networks (GANs). Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) are a first attempt at creating generative models.  In the context of games, GANs are modeled as a two player adversarial games. One of the biggest challenges faced with supervised learning is annotating the data. We cannot annotate automatically and without annotations we cannot train our learning models. But what if we could substitute the annotation of the data for something else? For instance, what if we could model the annotation task as a game or use other previous knowledge about the world as labels. These ideas are one of the main motivations for GANs. 
GANs are deep neural networks that consist of a generator network connected to a discriminator network. The discriminator network has training data and the generator network only has random or noise data as input. GANs are essentially 2 player games where one player (the generator) creates synthetic data samples, while the second player (the discriminator) takes the generated sample and performs a classification. 
This classification is performed to determine if the synthetic sample is similar to the distribution of the discriminator's training data. Since both networks are connected, the deep neural network (GAN) can learn to generate better synthetic samples with the help of the discriminator’s feedback. Basically, the discriminator tells the generator how to adjust its weights to produce better synthetic samples. 

Generative Adversarial Networks are methods that use 2 deep neural networks to interact with each other and generate data. Its formulation is consistent with 2 player adversarial game frameworks. One of the 2 algorithms (or networks) tries to learn a data distribution and produce new samples similar to the samples in the real data (the generator). The second algorithm (the discriminator) is a classifier that tries to determine if the new samples generated by the generative algorithm are fake or real. These 2 algorithms work together to achieve an optimal outcome of producing better output samples from the Generator. 


The generator in a GAN is based on Auto-encoders. Therefore, before looking at GANs, we will look at the Auto-encoder
</p>




<h1>Copyright, License, FTC and Amazon Disclaimer</h1>

<p>
 Copyright &copy by Ricardo A. Calix. <br/>
 All rights reserved. No part of this work may be reproduced or transmitted in any form or by any means, without written permission of the copyright owner. <br/>
 This post/page/article includes Amazon Affiliate links to products. This site receives income if you purchase through these links. 
 This income helps support content such as this one. 
 <br/>

	

  
</p>

     <center>
      <div class="img"> 
        <a href="https://amzn.to/3vOL8NF"><img src="https://m.media-amazon.com/images/I/71Wi+z5fKzL._SL1233_.jpg" height="500" width="auto"></a>
      </div>

    </center>
    

<h1>

	Generative Adversarial Networks
</h1>




\chapter{Convolutional Neural Networks}



In this this chapter, I will focus on a more advanced topic in deep learning called Convolutional Neural Networks (CNNs). This type of technique has been used extensively for image processing. It has the capability to learn the features from the image data without human intervention. The explanations in this chapter assume that you have read previous chapters of this book. I will re-use a lot of the code we previously used to define our logistic regression and deep neural network algorithms. There will be a few new functions to define a convolutional network architecture and to perform some new operations but you will notice how much of the CNN code is similar to what we have done in previous chapters.  


\section{The Data}

CNNs have traditionally been used on image data. The firs data set we will use to learn about CNNs is called the  MNIST dataset. It is a well known annotated dataset containing images of hand written digits in the range of 0, 1,..,9. The data set consists of a train set, a validation set, and a test set. It has around 70,000 images of dimension 28x28 in grey scale.


\section{Convolution and CNNs Defined}

So, what is a convolution? Convolution is a mathematical operation between two functions \textbf{f} and \textbf{g} to produce a new modified function \textbf{(f * g)}. It is a special kind of operation that involves the multiplication of 2 input functions with some additional conditions. As an example, in image processing this could mean the convolution between function \textbf{g} (an image) with a function \textbf{f} (a filter) to produce a new modified version of the image. 
Image processing uses filters to identify features in images such as for edge detection. Edge detecting filters, for instance, look for areas in an image of high variation to identify edges. That is, where the values of the pixels are all about the same may be considered a background but where the values are consistent and then start changing may mean that an edge is detected.

In the previous chapters, we have multiplied a data set matrix X with a weight matrix W. Applying a filter to an image is a similar process where you multiply (using a convolution operation) an image matrix (equivalent to X) with a filter matrix (similar to the weights). In fact, there are many types of filters that could be defined for image processing. In the past, these filters had to be defined by human feature engineers. The insight given by convolutional neural networks is that, given training data with labels, these filters (the convolution filters) can be learned by the model by learning the weights. And because the neural networks have multiple layers, convolutional filters learned from one layer can be used to transform inputs for the following layer. 

\section{Architecture for a Convolutional Neural Network with MNIST}

In this section, I will provide the main description of the architecture of the convolutional neural network and show some diagrams to better interpret the intuition of convolutional neural networks (CNNs). The diagram below shows an overview of the model we are going to build to perform image classification of hand written digits. 
The CNN we are going to use as our example consists of the following layers:

\begin{itemize}
    \item  input layer (the image)
    \item convolutional layer 1
    \item convolutional layer 2
    \item fully connected layer
    \item output layer
\end{itemize}



There are many details that could be described to define the architecture of a convolutional neural network. However, implementing one in PyTorch is not that difficult. Defining the architecture of a CNN is similar to how we defined the architecture of our previous deep learning classifiers. We need to define the number of layers and the size of each layer. In our previous layer definitions, the matrix multiplication was our main operation. When implementing a convolutional layer the main operation is a convolution. For convenience, here we will think of convolutional layers as black boxes of filters with inputs and outputs. Therefore, whenever we define a convolutional layer we need to define the following:
The number of inputs and the size of each input. In this case, the size of each input refers to the size of the image. In the case of Mnist, the images are 28*28 each. The number of inputs refers to the number of channels for the image. For instance, 1 channel for grey scale images and 3 channels for RGB or color images (color images are actually 3 matrices of size 28*28).
The number of filters  is a value that is defined by the network architect. For example, 24 filters or 16 filters. These are the convolutional filters which will be applied to the images. 




In the case of 16 filters, it means that 16 different filters would be applied to 1 input image to produce 16 new processed images (these new 16 processed versions of the input images would be referred to as producing 16 output channels). The size of the filter is also defined (for instance a filter of 5*5).
The number of outputs, as indicated in the previous bullet, refers to the output output channels and consist of the processed images after convolution. 



It is important to note that the convolution process is a bit more complicated when the number of inputs is more than 1; for example, when a convolutional layer has 16 input channels (16 versions of the input image) to the layer and 36 filters to be applied. In this case each input channel needs to be processed by all 36 filters. In the end, the layer will output 36 processed images. Additionally, it is important to note that the output images may not always retain their original size (e.g. 28*28 for mnist). Instead, after each layer, the processed images may be down sampled. This is called maxpooling. In our example, we will down sample from 28x28 to 14x14 and then to 7x7. 
So let us discuss the architecture of the CNN we are going to implement. The CNN will have the following characteristics. 

1 input layer, 2 convolutional layers, 1 fully connected layer, and 1 output layer



The input layer will consist of images of 28x28 with 1 channel
The first convolutional layer will have 16 filters. Each filter will be of size 5x5. The images will be down sampled to 14x14.
The second convolutional layer will have 36 filters. The filters will be 5x5. The images will be down sampled to 7x7. 
The fully connected layer will have 1024 neurons. This is a normal layer that will connect the output of the second convolutional layer to the output layer.
The output layer has 10 nodes which represent the 10 classes in the MNIST dataset. 



\begin{figure}[H]\centering
\adjustbox{max height=.70\textheight}{
    \includegraphics{images/CNN_architecture.png}
}
\caption{CNN architecture for MNIST}
\label{RegLin:fig}
\end{figure}




Based on the previous characteristics (see figure above), we can define our network architecture dimensions as follows:

\begin{itemize}
    \item Input layer: 28x28x1
    \item Conv layer 1 output: 14x14x16
    \item Conv layer 2 output: 7x7x36
    \item Fully connected layer: 1024
    \item Output layer: 10
\end{itemize}



The figure above presents a more visual representation of our example convolutional neural network.

\section{Coding a CNN for MNIST}

Once we have defined the architecture, we are ready to start coding our CNN. The code in this chapter is very similar to code from previous chapters. Therefore, I will only focus on new aspects of the code and will try not to repeat descriptions that have been provided in previous chapters.  
We can use the following libraries



\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={CNN libraries}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

import torch
import numpy as np
import os
from torchvision import datasets
from torchvision import transforms
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
import torch.optim as optim 
import torch.nn as nn

\end{lstlisting}
\end{minipage}




The first part of the code that needs to be defined is the section used to set the algorithm parameters.


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={CNN parameters}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


learning_rate = 0.003
N_Epochs      = 20
batch_size    = 32

\end{lstlisting}
\end{minipage}


Most of the parameters defined in this section are similar to parameters we have used in previous chapters. A new parameter is the dropout. The dropout is a parameter for a technique first defined by Geoffrey Hinton. Dropout is a technique that helps to perform a better optimization during the weight search. Every iteration during training, the dropout percent of connections (weights) is dropped. 



At the heart of a CNN there are 2 main operations which are the convolution and the maxpool operation. The convolution code can be seen below. As can be seen, it takes images and filters and performs a convolution operation. The strides parameter defines how the filter will slide across the image (e.g. every pixel, every 2 pixels, etc.). Therefore, strides can achieve the same objective and maxpooling. For instance, a stride of 2 can be seen as maxpooling the image by a factor of 2 as well. Both approaches in the literature have been found to achieve similar results. 


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Convolution and maxpool operations}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


model =  nn.Sequential(
            
    ## Convolution layer 1
    nn.Conv2d(3, 16, kernel_size=5, stride=1),
    nn.LeakyReLU(0.2),
    nn.BatchNorm2d(16),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),
      
    ## Convolution layer2
    nn.Conv2d(16, 32, kernel_size=5, stride=1),
    nn.LeakyReLU(0.2),
    nn.BatchNorm2d(32),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),
        
)


\end{lstlisting}
\end{minipage}



The maxpool or stride operation are used to down sample the images. For instance, in the case of the mnist images which have a size of 28x28, every time the images or their filtered equivalents pass through a maxpool function (or use stride), the result is that the image is reduced in size. During the first convolution (convolution layer 1), the filtered images are down sampled from 28x28 to 14x14. In this MNIST example, I will use maxpool which means I also need to set the strive value to one.

In the previous code listing, the activation is ReLU, and normalization and dropout layers can also be seen. 


To keep things simple here, I will read the MNIST from the datasets module in torchvision. 

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Read the MNIST data}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


data_path = "data/MNISTdata/"

mnist_train = datasets.MNIST(data_path, train=True,  download=True)
mnist_test  = datasets.MNIST(data_path, train=False, download=True)


\end{lstlisting}
\end{minipage}

The datasets module also has a nice way of applying filters to the data so that it can be converted to Torch tensors. Many filter can be applied but here I only use the \textbf{Transforms.ToTensor()} which is necessary to convert the whole dataset to torch tensors.

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Transformes to convert to torch tensors}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


mnist_train_tr = datasets.MNIST(data_path, train=True, download=False, 
                                            transform=transforms.Compose([
                                                transforms.ToTensor()
                                            ]))
                                            
mnist_test_tr  = datasets.MNIST(data_path, train=False, download=False, 
                                            transform=transforms.Compose([
                                                transforms.ToTensor()
                                            ]))


\end{lstlisting}
\end{minipage}

we can now proceed to display an image with the PIL image module



\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Display an image}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


Image.fromarray(  mnist_test_tr.data[337].numpy()  ) 


\end{lstlisting}
\end{minipage}

which results in the following

\begin{figure}[H]\centering
\adjustbox{max height=.55\textheight}{
    \includegraphics{images/MNIST_IMAGE.png}
}
\caption{An image from MNIST}
\label{RegLin:fig}
\end{figure}

It is always a good idea to check the tensor shapes. The following code gives us a shape of [60000, 28, 28] for the train set, and of [10000, 28, 28] for the test set. Torch convolution layers prefer shapes like [10000, 1, 28, 28] instead of like this [10000, 28, 28]. The Dataloader takes care of that so we do not have to change it manually. 

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Check Tensor shapes}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]


## mnist_train_tr.data = mnist_train_tr.data.view(60000, 1, 28, 28)

mnist_train_tr.data.shape

mnist_test_tr.data.shape

\end{lstlisting}
\end{minipage}

Here we create the DataLoaders as follows: 

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={DataLoaders for MNIST}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

batch_size = 32

train_dl  = torch.utils.data.DataLoader(mnist_train_tr, batch_size=batch_size, shuffle=True  ) 


test_dl   = torch.utils.data.DataLoader(mnist_test_tr,  batch_size=10000,      shuffle=False ) 

\end{lstlisting}
\end{minipage}


Next, we add our familiar classifications performance function from previous chapters


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Performance metrics function}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

def print_metrics_function(y_test, y_pred):
    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)
    print("Confusion Matrix:")
    print(confmat)
    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))
    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))
    f1_measure = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')
    print('F1-mesure: %.3f' % f1_measure)
    return f1_measure
    

\end{lstlisting}
\end{minipage}


Now we are ready to implement our CNN architecture for the MNIST data as follows


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={CNN NN architecture}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

class Classifier_CNN(nn.Module):
    
    def __init__(self):
        
        super().__init__()
            
        self.model = nn.Sequential(
                
                ## Convolitional Layer 1
                nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2, 2), 
 
                ## Convolutional Layer 2
                nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2, 2),   
 
                ## feed forward layer 
                nn.Flatten(),
                nn.Linear(800, 1024),    ## see how to get 800 below on last cell
                nn.ReLU(),

                nn.Linear(1024, 10),
                nn.LogSoftmax(dim=1)
        )
 
 
           
    def forward(self, inputs):
            
        return self.model(inputs)
    
    
\end{lstlisting}
\end{minipage}


Now we can proceed to define our familiar training loop


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={CNN training loop}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

def training_loop( N_Epochs, model, loss_fn, opt  ):
    
    losses_list = []
    
    for epoch in range(N_Epochs):
        for xb, yb in train_dl:
            
            ## print( xb.shape )   ## check this comes out as [N, 1, 28, 28]
            ## yb = torch.squeeze(yb, dim=1)
            
            y_pred = model(xb)
            loss   = loss_fn(y_pred, yb)
       
            opt.zero_grad()
            loss.backward()
            opt.step()
            
        if epoch % 1 == 0:
            print(epoch, "loss=", loss)
            losses_list.append(  loss  )
            
    return losses_list
    
    
\end{lstlisting}
\end{minipage}


That is it as far as preparing the data and defining the classes. We are now ready to initialize the core functions and to start the training process. We can do so as follows 

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Call the core functions for training}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

model          = Classifier_CNN()

opt            = torch.optim.Adam(    model.parameters(), lr=learning_rate )

loss_fn        = nn.CrossEntropyLoss( )   

my_losses_list = training_loop(  N_Epochs, model, loss_fn, opt  )
    
    
\end{lstlisting}
\end{minipage}


Running the previous code start the training and we can see the losses here

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={The losses during training}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

0  loss= tensor(0.0035, grad_fn=<NllLossBackward0>)
1  loss= tensor(0.0191, grad_fn=<NllLossBackward0>)
2  loss= tensor(0.0094, grad_fn=<NllLossBackward0>)
3  loss= tensor(0.0857, grad_fn=<NllLossBackward0>)
4  loss= tensor(0.0012, grad_fn=<NllLossBackward0>)
5  loss= tensor(0.0007, grad_fn=<NllLossBackward0>)
6  loss= tensor(1.3709e-06, grad_fn=<NllLossBackward0>)
7  loss= tensor(0., grad_fn=<NllLossBackward0>)
8  loss= tensor(0.0128, grad_fn=<NllLossBackward0>)
9  loss= tensor(0.0200, grad_fn=<NllLossBackward0>)
10 loss= tensor(0.0070, grad_fn=<NllLossBackward0>)
11 loss= tensor(0.0022, grad_fn=<NllLossBackward0>)
12 loss= tensor(5.8149e-06, grad_fn=<NllLossBackward0>)
13 loss= tensor(2.6077e-08, grad_fn=<NllLossBackward0>)
14 loss= tensor(3.3155e-07, grad_fn=<NllLossBackward0>)
15 loss= tensor(3.7253e-09, grad_fn=<NllLossBackward0>)
16 loss= tensor(6.4162e-05, grad_fn=<NllLossBackward0>)
17 loss= tensor(0.0002, grad_fn=<NllLossBackward0>)
18 loss= tensor(0.2447, grad_fn=<NllLossBackward0>)
19 loss= tensor(0.0643, grad_fn=<NllLossBackward0>)
    
\end{lstlisting}
\end{minipage}


After training we would like to predict and evaluate the  trained model on the test set. This can be achieved with the following code

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Code to evaluate performance}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

with torch.no_grad():
    for x_real, y_real in test_dl:
        
        y_pred = model(  x_real  )
        
        vals, indeces = torch.max( y_pred, dim=1  )
        preds = indeces
        print_metrics_function(y_real, preds)
    
\end{lstlisting}
\end{minipage}

which gives us

\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Performance metrics}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

Accuracy: 0.99
Confusion Matrix:
[[ 974    0    1    2    0    0    2    1    0    0]
 [   0 1130    1    2    0    0    1    1    0    0]
 [   0    1 1024    0    1    0    1    1    4    0]
 [   0    0    2 1000    0    3    0    0    3    2]
 [   0    1    0    0  974    0    1    0    3    3]
 [   0    1    0    7    0  880    1    2    1    0]
 [   4    3    0    0    0    3  945    0    3    0]
 [   0    1   11    1    2    0    0 1011    0    2]
 [   0    0    0    1    0    1    0    2  968    2]
 [   2    1    0    0    8    1    0    8    4  985]]
Precision: 0.989
Recall: 0.989
F1-mesure: 0.989
    
\end{lstlisting}
\end{minipage}

As can be seen for the previous results, the code performed really well. 

\subsection{Figuring out the tensor shapes after the convolution layers}

Figuring out the tensor shapes after the convolutional and maxpool layers can be challenging. One way to find out what it is is to run a dummy tensor through the CNN architecture and print the shape after all the convolutions. 
This can be achieved with the following code:


\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{showstringspaces=false}  %% remove weird symbol in spaces
\lstset{frame=lines}
\lstset{caption={Tensor shape after convolution layers}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

N_batches = 10

model_rc = nn.Sequential(    
    ## Convolitional Layer 1
    nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),     
 
    ## Convolutional Layer
    nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),     
    
    nn.Flatten()
)

for xb, yb in train_dl:
    print( xb.shape )    
    break

## [32, 1, 28, 28]

my_tensor_test   = torch.randn(N_batches, 1, 28,  28)
res_actual_model = model_rc(  my_tensor_test   )
print( res_actual_model.shape )

## torch.Size([10, 800])
    
\end{lstlisting}
\end{minipage}

As can be seen, the previous code listing shows an example of how we can determine the shape of our output tensor after the convolutional layers and the flatten operation. We can see that the value is 800 which  is the same size we defined in our CNN architecture after the flatten operation. 


\section{CNNs for RGB Data with your own images}

In the previous section we looked at CNNs for grey scaled data. In this section we apply CNNs to color images. 

\begin{comment}


The main change will relate to the fact that the inputs for grey scale are [n, m, 1] whereas inputs for color images are [n, m, 3].
Next we describe the code. The data can be downloaded from the github. First we add the libraries as usual. 


Next we set parameters to remove warnings and to see all values when printing numpy arrays.
 


Here we set the parameters for the loop. Notice that we train for 200,000 epochs. We read the data in batches of 128. 


Now it starts to get interesting. We need to set the size of the images. In MNIST our images where of size 28x28x1. In this example we use color images of size 100x100x3 (data available on the github). Given the size of each image, our number of features is 30,000 or 100x100x3. It is important that you are aware that data is fed as tensors into the computational graph. Therefore, the images or tensors may be reshaped from cubes to vectors and vice versa as needed. The number of classes for this data set is 4 given that we are trying to classify between 4 fruits. The last parameter is the dropout. Remember dropout forces the network to use fewer neurons. 



Now we are ready to read in our data. We use PIL. Put your images in a folder named testA. We can use glob to gather the images from the folder and then PIL to read them in. The images are grouped by label into 4 different folders.  Two list for images and labes are used to store the data. I left the commented out lines so you can play with visualizing the images as you read them in. 



The way I read the images uses 4 loops for each folder. This could have been done in many different ways. I simly thought this was the clearest approach. 


The following reads in data for bananas. 


Now we read in pepino jpgs. 


Finally, we read in peppers. 


After reading the images, we convert the lists to numpy arrays and visualize dimensions.



You could reshape the images for various reasons. Here is an example of how to do that. Try it. 

After converting the images to numpy arrays, you can save them to file for later use. 


For our purposes, I renamed the matrix as your\_mnist so you know this is like mnist would be. Remember the data is now about fruits. 



\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Numpy math operations}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

x = np.array([1,2,3,4])

print(x+10)
print(x-10)
print(x*10)
print(x/2)
print(-x)
print(x ** 3) 
print(np.power(4, x)) 
print(np.log(x))
print(np.log2(x)) 
print(np.log10(x) )

\end{lstlisting}
\end{minipage}



Remember to always normalize. 


Now we perform train and test split as we previously learned. 


Here we define the performance metrics function for precision, recall, f-measure, and the confusion matrix. 


The plotting function. 


Now we define the concolution function with strides equal to 1 and a relu function. Remember that relu removes negative values and looks like a hokey stick. Also, remember relu is not constrained to ranges such as [0..1] or [-1..1] but instead [0..infinity]. Therefore, is has more power to learn because the positive values range is much wider and has more power to learn. 



The max pooling function. This problem of images that are 100x100x3 results in a max pooling situation where the images have dimensions that are uneven numbers. How do we resolve this? Easily. We use padding=SAME. So when the image is image 25x25 maxpool would give an image that is -> 12.5 x 12.5. Because of padding=same then it is rounded up to 13x13. Problem solved! 



Now we define the layer function as previously discussed which is used for the fully connected layer. 



We define the convolutional layer function using our previous conv2d and max pool functions. 



Next we define the fully connected layer. Notice that the -1 in the reshape allows to infer the size of that dimension.



Finally, we are ready to define the architecture. This is what you will usually need to figure out depending on your images. So we first reshape each input picture in the batch. Notice that the batch itself is a tensor of n images. 

The shape is equal to 

[-1, size\_image\_x, size\_image\_y, 3 channels (e.g. rgb)]

The resized image for rgb and batches of 128 would be [128, 30000] because there are 128 samples per batch and images are 100x100x3 = 30000. This has to be re-shaped bacause Convolutional layers only take 4 dimensional tensors as     input. 

The -1 infers the number of batches and then we make the 30,000 into 100x100x3. In this first function we start with 16 filters. 








\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Numpy math operations}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

x = np.array([1,2,3,4])

print(x+10)
print(x-10)
print(x*10)
print(x/2)
print(-x)
print(x ** 3) 
print(np.power(4, x)) 
print(np.log(x))
print(np.log2(x)) 
print(np.log10(x) )

\end{lstlisting}
\end{minipage}











The next code segment is another version of the previous function. Here we start with 32 filters instead of 16. 



Finally, in this function we use 3 convolutions instead of 2 like in the previous functions.






\begin{minipage}{\linewidth}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Numpy math operations}}
%\lstset{label={lst:code\\_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[backgroundcolor = \color{white}]

x = np.array([1,2,3,4])

print(x+10)
print(x-10)
print(x*10)
print(x/2)
print(-x)
print(x ** 3) 
print(np.power(4, x)) 
print(np.log(x))
print(np.log2(x)) 
print(np.log10(x) )

\end{lstlisting}
\end{minipage}







So, that was the hard part. Now we are ready for the loss function. It is a simple cross entropy as previously described. 



We train with the Adam optimizer. 



The standard accuracy function. 





Now we are ready for the place holders. Notice that the image cubes are flatten from 3 dimensions to just 1. There are alternatives to this.  



Now we are ready to call the core functions. 



Remember this converts from one hot encoding back to a 1 column class vector. 





This should be familiar by now. We initialize the session and variables. 


Here we one hot encode the labels. Remember that there are 4 fruits so we have 4 classes. 



Next we calculate the batch processing parameters. 




We can modify the dropout parameter here. 


Finally, we made it to the main loop.


That is it. 




\end{comment}



\section{ Summary}

In this chapter, an implementation of a CNN model for hand written digit identification was presented and discussed. The code was provided and results of the classification task were also presented and discussed. The CNN used the MNIST data set for inputs and focused on building deep neural networks with several convolution and maxpool layers. The architecture consisted of 2 convolutional layers followed by 1 fully connected layer of size 1024. 





	
<p>

In this section of the book I will cover Generative Adversarial Networks (GANs). Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) are 
	a first attempt at creating generative models.  In the context of games, GANs are modeled as a two player adversarial games. One of the biggest challenges
	faced with supervised learning is annotating the data. We cannot annotate automatically and without annotations we cannot train our learning models. But what 
	if we could substitute the annotation of the data for something else? For instance, what if we could model the annotation task as a game or use other previous
	knowledge about the world as labels. These ideas are one of the main motivations for GANs. 
GANs are deep neural networks that consist of a generator network connected to a discriminator network. The discriminator network has training data and the generator 
	network only has random or noise data as input. GANs are essentially 2 player games where one player (the generator) creates synthetic data samples, while the 
	second player (the discriminator) takes the generated sample and performs a classification. 
This classification is performed to determine if the synthetic sample is similar to the distribution of the discriminator's training data. Since both networks are 
	connected, the deep neural network (GAN) can learn to generate better synthetic samples with the help of the discriminator’s feedback. Basically, the discriminator 
	tells the generator how to adjust its weights to produce better synthetic samples. 
<br />
Generative Adversarial Networks are methods that use 2 deep neural networks to interact with each other and generate data. Its formulation is consistent with 2 player
	adversarial game frameworks. One of the 2 algorithms (or networks) tries to learn a data distribution and produce new samples similar to the samples in 
	the real data (the generator). The second algorithm (the discriminator) is a classifier that tries to determine if the new samples generated by the generative 
	algorithm are fake or real. These 2 algorithms work together to achieve an optimal outcome of producing better output samples from the Generator. 

<br />
The generator in a GAN is based on Auto-encoders. Therefore, before looking at GANs, we will look at the Auto-encoder
	
</p>

<h1>
Autoencoders
	
</h1>

<p>

Autoencoders are a type of compression method where a neural network learns how to represent a vector of size “m” into a vector of size “n” where m >> n. 
	Here, the input and output vectors in the network are the original sample and the reproduced sample and the hidden layer of the network is the new
	compressed representation of the input vector. The objective function minimizes the difference/distance between the original input sample and the reproduced 
	output sample.  
	
</p>

<h1>
Original GAN
	
</h1>

<p>
The original GAN consists of a generator and a Discriminator and was proposed in a 2014 paper by Ian Goodfellow. The general architecture of the GAN can be seen
	in the figure below.

	
</p>

  <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/ganNN.300dpi.jpg" height="400" width="auto">
      </div>

    </center>	





<h1>
Generating MNIST digits with GANs
	
</h1>

<p>
In this section, I will describe how to implement a GAN that can generate images. The algorithm will work with the MNIST data set. As always, 
	the code can be downloaded from my GitHub. 
<br/>
First we import the libraries. 


	
</p>


<center>
<div>
<textarea rows="20" cols="100">

import torch
import numpy as np
import os
from torchvision import datasets
from torchvision import transforms
import torchvision.transforms as T
import matplotlib.pyplot as plt
import pandas as pd
from numpy import genfromtxt
from PIL import Image
import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
from mlxtend.plotting import heatmap
from sklearn.model_selection import train_test_split
from mlxtend.plotting import heatmap
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim 
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable


</textarea>
  
</div>
</center>






     


Next we can define the parameters as follows



<center>
<div>
<textarea rows="7" cols="100">

learning_rate    = 0.003  ## Adam default   ## 0.001
batch_size       = 32
N_Epochs         = 30  ##27000  

</textarea>
  
</div>
</center>



<p>
We load the MNIST data in a similar way to what we did in previous chapters

	
</p>

<center>
<div>
<textarea rows="16" cols="100">

data_path   = "data/MNISTdata/"
mnist_train = datasets.MNIST(data_path, train=True, download=True)
mnist_test = datasets.MNIST(data_path, train=False, download=True)

mnist_train_tr = datasets.MNIST(data_path, train=True, download=False, 
                                            transform=transforms.Compose([
                                                transforms.ToTensor()
                                            ]))
                                            
mnist_test_tr  = datasets.MNIST(data_path, train=False, download=False, 
                                            transform=transforms.Compose([
                                                transforms.ToTensor()
                                            ]))

</textarea>
  
</div>
</center>





     


It is a good idea to print the shapes of the tensors before creating the DataLoaders.

<center>
<div>
<textarea rows="16" cols="100">

mnist_train_tr.data.shape
## [60000, 28, 28]

mnist_test_tr.data.shape
## [10000, 28, 28]

train_dl  = torch.utils.data.DataLoader(mnist_train_tr, batch_size=batch_size, shuffle=True  ) 

test_dl   = torch.utils.data.DataLoader(mnist_test_tr,  batch_size=batch_size, shuffle=False ) 


</textarea>
  
</div>
</center>





     


Now we are ready to define the GAN architectures. GANs have a Generator and a Discriminator

In the following code segment we define the architecture for the Generator. Notice that this will be a neural network of size 100x256x784. 

<center>
<div>
<textarea rows="25" cols="100">

class Generator_Net(nn.Module):
    
    def __init__(self):
        super().__init__()
        

        self.linear1 = nn.Linear(100, 256)
        self.act1    = nn.LeakyReLU(0.02)
        self.norm1   = nn.LayerNorm(256)
        self.linear2 = nn.Linear(256, 784)
        self.act2    = nn.Sigmoid()
        self.dropout = nn.Dropout(0.25)
        
    def forward(self, rand_input ):
        

        x      = self.linear1( rand_input )
        x      = self.act1(x)
        x      = self.norm1(x) 
        x      = self.linear2(x)
        x      = self.act2(x)
        y_pred = x
        
        return y_pred

     
</textarea>
  
</div>
</center>





I also tried a deeper architecture for the GAN but was not able to get it to learn in 30 epochs as I did with the simple MLP GAN. I am including it here and leave it as a exercise for the reader. 


<center>
<div>
<textarea rows="25" cols="100">


class Generator_DL_Net(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.linear1 = nn.Linear(100, 60)
        self.act1    = nn.LeakyReLU(0.02)
        self.norm1   = nn.LayerNorm(60)
        self.linear2 = nn.Linear(60, 120)
        self.act2    = nn.LeakyReLU(0.02)
        self.norm2   = nn.LayerNorm(120)
        self.linear3 = nn.Linear(120, 784)
        self.act3    = nn.Sigmoid()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, rand_input ):
        

        x      = self.linear1( rand_input )
        x      = self.act1(x)
        x      = self.norm1(x) 
        x      = self.dropout(x)
        x      = self.linear2(x)
        x      = self.act2(x)
        x      = self.norm2(x) 
        x      = self.dropout(x)
        x      = self.linear3(x)
        x      = self.act3(x)
        
        y_pred = x
        
        return y_pred

     

</textarea>
  
</div>
</center>
	




The architecture for the Discriminator can be seen in the next code segment.



<center>
<div>
<textarea rows="25" cols="100">

class Discriminator_Net(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.linear1 = nn.Linear(784, 100)
        self.act1    = nn.ReLU()
        self.linear2 = nn.Linear(100, 50)
        self.act2    = nn.ReLU()
        self.linear3 = nn.Linear(50, 1)
        self.act3    = nn.Sigmoid()             ## nn.Softmax(dim=1)
        self.dropout = nn.Dropout(0.25)
        

    def forward(self, x):
        
        x      = self.linear1(x)
        x      = self.act1(x)
        x      = self.dropout(x)
        x      = self.linear2(x)
        x      = self.act2(x)
        x      = self.dropout(x)
        x      = self.linear3(x)
        y_pred = self.act3(x)
        
        return y_pred

     
</textarea>
  
</div>
</center>
	






Notice that the architecture for this network is 784x100x50x1. Its input is an image vector (real or fake) and its output is one neuron with value 0 or 1. 

The following function can be used to generate seed random noise vectors for the Generator input. For training we want batches of noise vectors.



<center>
<div>
<textarea rows="9" cols="100">

def random_G_batch_vector_input():
    rand_vec = torch.randn( (batch_size, 100 ) )
    return rand_vec

    
</textarea>
  
</div>
</center>
	









To generate individual images, we can use the following function to generate seed noise vectors



<center>
<div>
<textarea rows="9" cols="100">

def random_G_vector_input():
    rand_vec = torch.randn( 100 )
    return rand_vec
    
</textarea>
  
</div>
</center>






     


The Training function for the GAN is the most complicated we have seen so far. The full code can be seen seen below. As can be seen, we train the discriminator twice and the generator once. The Discriminator looks at a real image and it should predict that it is real (a one). Then the discriminator looks at a generated image (fake) and it should predict that it is a fake (a zero). The discriminator weights should be updated accordingly for these objectives. The final step is to update the weights of the Generator. Here, we want to trick the discriminator. So now the generated image (fake) is given to the Discriminator but we want it to say that it is real (a one). We do this using the loss of the Discriminator but adjust the weights of the Generator. So the Generator weights are updated in such a way that it generates images that trick the Discriminator into predicting that the fake images are true (a one). 

Notice that before training, we need to squeeze and reshape the input \textbf{xb} tensor from [batch, 1, 28, 28] to [batch, 784] . We can do that with the following statements.

<center>
<div>
<textarea rows="6" cols="100">

xb = torch.squeeze(xb, dim=1)
            
xb = xb.reshape((-1, 784))

    
</textarea>
  
</div>
</center>
	


<p>

	and the GAN training function is
</p>
     

<center>
<div>
<textarea rows="35" cols="100">

	
list_losses_real    = []
list_losses_fake    = []
list_losses_tricked = []
def training_loop(  N_Epochs, G_model, D_model, D_loss_fn, G_opt, D_opt   ):
    for epoch in range(N_Epochs):
        for xb, yb in train_dl:              ## xb = [batch, 1, 28, 28]
            xb = torch.squeeze(xb, dim=1)
            xb = xb.reshape((-1, 784))
            #################################################
            ## G_model.eval()     ## No G training
            ## gen_img = G_model( random_G_vector_input() )
            gen_img = G_model( random_G_batch_vector_input() ).detach()
            ## Train D with real data
            D_real_y_pred = D_model(  xb  )
            D_real_loss   = D_loss_fn( D_real_y_pred, torch.ones((batch_size, 1)) )
            D_opt.zero_grad()
            D_real_loss.backward()
            D_opt.step()
            ## Train D with fake data
            D_fake_y_pred = D_model(  gen_img  )
            D_fake_loss   = D_loss_fn( D_fake_y_pred, torch.zeros((batch_size, 1)))
            D_opt.zero_grad()
            D_fake_loss.backward()
            D_opt.step()
            ## G_model.train()    ## yes G training
            #################################################
            ## D_model.eval()     ## No D training
            ## gen_img = G_model( random_G_vector_input() )
            gen_img = G_model( random_G_batch_vector_input() )
            ## Train G with D_loss (need to trick D)
            D_tricked_y_pred = D_model(  gen_img  )
            D_tricked_loss   = D_loss_fn( D_tricked_y_pred, torch.ones((batch_size, 1)) )
            G_opt.zero_grad()
            D_tricked_loss.backward()
            G_opt.step()
            ## D_model.train()    ## yes D training
        if epoch % 1 == 0:
            print("******************************")
            print(epoch, "D_real_loss=", D_real_loss)
            print(epoch, "D_fake_loss=", D_fake_loss)
            print(epoch, "D_tricked_loss=", D_tricked_loss)
            list_losses_real.append(        D_real_loss.detach().numpy()  )
            list_losses_fake.append(        D_fake_loss.detach().numpy()  )
            list_losses_tricked.append(  D_tricked_loss.detach().numpy()  )
            


    
</textarea>
  
</div>
</center>




Finally, we can call the core functions and print the losses during training. 

<center>
<div>
<textarea rows="12" cols="100">

G_model     = Generator_Net()

## G_model     = Generator_DL_Net()

D_model     = Discriminator_Net()

## D_loss_fn   = nn.CrossEntropyLoss( )  
## D_loss_fn   = F.mse_loss

D_loss_fn   = nn.BCELoss()

G_opt       = torch.optim.Adam( G_model.parameters(), lr=learning_rate )
D_opt       = torch.optim.Adam( D_model.parameters(), lr=learning_rate )

training_loop(  N_Epochs, G_model, D_model, D_loss_fn, G_opt, D_opt )


</textarea>
  
</div>
</center>





We can seen below that the losses are going down for both the Generator and Discriminator. 

<center>
<div>
<textarea rows="25" cols="100">

******************************
0 D_real_loss= tensor(0.1157, grad_fn=<BinaryCrossEntropyBackward0>)
0 D_fake_loss= tensor(0.0114, grad_fn=<BinaryCrossEntropyBackward0>)
0 D_tricked_loss= tensor(7.8405, grad_fn=<BinaryCrossEntropyBackward0>)
******************************
1 D_real_loss= tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward0>)
1 D_fake_loss= tensor(0.0604, grad_fn=<BinaryCrossEntropyBackward0>)
1 D_tricked_loss= tensor(5.4396, grad_fn=<BinaryCrossEntropyBackward0>)
******************************
2 D_real_loss= tensor(0.1128, grad_fn=<BinaryCrossEntropyBackward0>)
2 D_fake_loss= tensor(0.0804, grad_fn=<BinaryCrossEntropyBackward0>)
2 D_tricked_loss= tensor(4.2457, grad_fn=<BinaryCrossEntropyBackward0>)

...

******************************
27 D_real_loss= tensor(0.6145, grad_fn=<BinaryCrossEntropyBackward0>)
27 D_fake_loss= tensor(0.3502, grad_fn=<BinaryCrossEntropyBackward0>)
27 D_tricked_loss= tensor(1.2594, grad_fn=<BinaryCrossEntropyBackward0>)
******************************
28 D_real_loss= tensor(0.4972, grad_fn=<BinaryCrossEntropyBackward0>)
28 D_fake_loss= tensor(0.3535, grad_fn=<BinaryCrossEntropyBackward0>)
28 D_tricked_loss= tensor(1.2053, grad_fn=<BinaryCrossEntropyBackward0>)
******************************
29 D_real_loss= tensor(0.5411, grad_fn=<BinaryCrossEntropyBackward0>)
29 D_fake_loss= tensor(0.5512, grad_fn=<BinaryCrossEntropyBackward0>)
29 D_tricked_loss= tensor(1.2962, grad_fn=<BinaryCrossEntropyBackward0>)

     

</textarea>
  
</div>
</center>





Using the following function, we can plot the losses. 

<center>
<div>
<textarea rows="15" cols="100">

def plot_GAN_losses(list_losses_real, list_losses_fake, list_losses_tricked):
    
    the_epochs = [i for i in range(len(list_losses_real))]  

    plt.plot(the_epochs, list_losses_real,    label = "real") 
    plt.plot(the_epochs, list_losses_fake,    label = "fake") 
    plt.plot(the_epochs, list_losses_tricked, label = "tricked")
    plt.legend() 
    plt.show()

plot_GAN_losses(list_losses_real, list_losses_fake, list_losses_tricked)


</textarea>
  
</div>
</center>






     


We can see that the losses of the Generator did not go below the discriminator loss. A good objective would be for them to be equal. 

	
  <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/GAN_losses_plot.png" height="400" width="auto">
      </div>

    </center>	



And that is it. The GAN is now trained. We can now proceed to test it and generate a few images. We can do that with the following code segment. 

<center>
<div>
<textarea rows="15" cols="100">

gen_test_img3 = G_model( random_G_vector_input() )
gen_test_img3 = gen_test_img3.reshape( (28,28) )
plt.imshow( gen_test_img3.detach().numpy() )
plt.show()

</textarea>
  
</div>
</center>





     


I did this several times and the results of the generated images are as follows. This first image looks like a bad zero. 

 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/bad_zero_gan.png" height="400" width="auto">
      </div>

    </center>
	



This next image looks like a better version of the zero.

 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/good_zero_gan.png" height="400" width="auto">
      </div>

    </center>



And I believe this looks like a five.


 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/good_5_gan.png" height="400" width="auto">
      </div>

    </center>



<h1>
Conditional GANs
	
</h1>

<p>

The conditional GAN  (CGAN) can generate more than random images from a distribution. Instead, in the case on MNIST, for example, 
	it can generate the image of an image given the corresponding label for the image. The architecture for the CGAN can be seen below.
	
</p>


	 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/ConditionalGanDiagram.jpg" height="400" width="auto">
      </div>

    </center>



<h1>
Summary
</h1>
	
<p>
In this chapter, a description of Generative Adversarial Networks was provided. Some sample code was addressed as well as some applications of GANs. 


	
</p>







</div>  <!-- for the fixed nav bar -->

    
  </body>
</html>
