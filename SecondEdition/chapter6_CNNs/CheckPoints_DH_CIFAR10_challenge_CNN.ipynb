{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CNNs for CIFAR10\n",
    "\n",
    "* https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/chapter6_CNNs/index.html\n",
    "\n",
    "## Load and save torch model checkpoints\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "## Data\n",
    "\n",
    "* Data: https://github.com/YoongiKim/CIFAR-10-images/tree/master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A30'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assign device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch_device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CIFAR10 DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_train = '/home/rcalix/Desktop/CIFAR-10-images-master/train/'\n",
    "\n",
    "raw_data_test  = '/home/rcalix/Desktop/CIFAR-10-images-master/test/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PATH to checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"/scratch/scholar/rcalix/CNN_model_CIFAR10\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = []\n",
    "labels_train  = []\n",
    "targets_train = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in os.listdir( raw_data_train ):\n",
    "    ## print(folder)\n",
    "    for image in os.listdir( os.path.join(raw_data_train, folder) ):\n",
    "        if folder not in labels_train:\n",
    "            labels_train.append( folder )\n",
    "        targets_train.append(  labels_train.index(folder)  )\n",
    "        img_arr = imageio.imread(  os.path.join(raw_data_train, folder, image), pilmode=\"RGB\"  )\n",
    "        \n",
    "        img = torch.from_numpy( img_arr ).permute( 2, 0, 1 ).float()\n",
    "        \n",
    "        img /= 255\n",
    "        dataset_train.append(img)\n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len( targets_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_train[3].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train    = torch.stack( dataset_train )\n",
    "targets_train = torch.Tensor(  targets_train  ).type(   torch.LongTensor   )\n",
    "\n",
    "torch.save(   (data_train, targets_train, labels_train), \"InClass_CIFAR10_data\"     )\n",
    "\n",
    "## data1, targets1, labels1 = torch.load(\"InClass_CIFAR10_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train[4].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_train[24000:25000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Print images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0235, 0.0196],\n",
       "         [0.0000, 0.0039, 0.0078,  ..., 0.0196, 0.0118, 0.0118],\n",
       "         [0.0157, 0.0039, 0.0000,  ..., 0.0078, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0314, 0.0353],\n",
       "         [0.0353, 0.0275, 0.0000,  ..., 0.0118, 0.0157, 0.0157],\n",
       "         [0.0000, 0.0235, 0.0000,  ..., 0.0157, 0.0235, 0.0275]],\n",
       "\n",
       "        [[0.0078, 0.0078, 0.0157,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0118, 0.0157, 0.0196,  ..., 0.0118, 0.0000, 0.0000],\n",
       "         [0.0157, 0.0039, 0.0039,  ..., 0.0078, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0157, 0.0118],\n",
       "         [0.0353, 0.0275, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0235, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],\n",
       "         [0.0078, 0.0000, 0.0000,  ..., 0.0157, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0078, 0.0078, 0.0078,  ..., 0.0235, 0.0196, 0.0196],\n",
       "         [0.0353, 0.0275, 0.0078,  ..., 0.0157, 0.0157, 0.0157],\n",
       "         [0.0000, 0.0235, 0.0078,  ..., 0.0196, 0.0235, 0.0275]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_tr = data_train[46000]\n",
    "img_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = T.ToPILImage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = transform(  img_tr  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x2B8EBE5A9610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Class balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train_np = targets_train.numpy() \n",
    "y_train_np.shape\n",
    "\n",
    "the_set = np.unique(  y_train_np  )\n",
    "the_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_ = plt.hist( targets_train.numpy() , bins=\"auto\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_test = []\n",
    "labels_test = []\n",
    "targets_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in os.listdir( raw_data_test ):\n",
    "    ## print(folder)\n",
    "    for image in os.listdir( os.path.join(raw_data_test, folder) ):\n",
    "        if folder not in labels_test:\n",
    "            labels_test.append( folder )\n",
    "        targets_test.append(  labels_test.index(folder)  )\n",
    "        img_arr = imageio.imread(  os.path.join(raw_data_test, folder, image), pilmode=\"RGB\"  )\n",
    "        \n",
    "        img = torch.from_numpy( img_arr ).permute( 2, 0, 1 ).float()\n",
    "        \n",
    "        img /= 255\n",
    "        dataset_test.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test   = torch.stack( dataset_test )\n",
    "targets_test = torch.Tensor(  targets_test  ).type(   torch.LongTensor   )\n",
    "\n",
    "torch.save(   (data_test, targets_test, labels_test), \"InClass_CIFAR10_data_test\"     )\n",
    "\n",
    "## data1, targets1, labels1 = torch.load(\"InClass_CIFAR10_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_ = plt.hist( targets_test.numpy() , bins=\"auto\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = data_train  \n",
    "y_train = targets_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = data_test  \n",
    "y_test = targets_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change to float 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.numpy()\n",
    "X_test  = X_test.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.astype(  np.float32  )\n",
    "X_test  = X_test.astype(   np.float32  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.from_numpy(X_train )\n",
    "X_test = torch.from_numpy( X_test  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_norm_mean = (0.5, 0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_norm_std = (0.5, 0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "other_normalization = transforms.Compose([\n",
    "                            ## transforms.ToTensor(),\n",
    "                            transforms.Normalize( img_norm_mean, img_norm_std )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = transforms.Compose([\n",
    "                 transforms.Resize(256),\n",
    "                 transforms.CenterCrop(224),\n",
    "                 transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = other_normalization( X_train )  \n",
    "\n",
    "X_test  = other_normalization( X_test ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Print shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train[30000].item()\n",
    "type(y_train[30000].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train[30000].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0980, -0.0980, -0.1137,  ..., -0.4902, -0.3333, -0.2941],\n",
       "         [-0.0275, -0.0039, -0.0118,  ..., -0.4275, -0.2784, -0.2784],\n",
       "         [-0.1294, -0.0824, -0.0588,  ..., -0.3804, -0.2784, -0.3490],\n",
       "         ...,\n",
       "         [-0.0980, -0.0745, -0.0902,  ..., -0.0588, -0.0980, -0.1294],\n",
       "         [-0.1216, -0.0902, -0.0980,  ..., -0.0667, -0.0902, -0.1137],\n",
       "         [-0.1686, -0.1059, -0.0824,  ..., -0.1137, -0.1216, -0.1294]],\n",
       "\n",
       "        [[ 0.0588,  0.0510,  0.0353,  ..., -0.4039, -0.2314, -0.1922],\n",
       "         [ 0.1137,  0.1451,  0.1373,  ..., -0.3412, -0.1765, -0.1765],\n",
       "         [ 0.0118,  0.0667,  0.0667,  ..., -0.2941, -0.1922, -0.2471],\n",
       "         ...,\n",
       "         [-0.0745, -0.0510, -0.0510,  ..., -0.0353, -0.0431, -0.0745],\n",
       "         [-0.0824, -0.0510, -0.0588,  ..., -0.0431, -0.0431, -0.0431],\n",
       "         [-0.1294, -0.0667, -0.0431,  ..., -0.0667, -0.0745, -0.0667]],\n",
       "\n",
       "        [[ 0.0510,  0.0353,  0.0196,  ..., -0.5373, -0.3882, -0.3490],\n",
       "         [ 0.1137,  0.1294,  0.1216,  ..., -0.4745, -0.3176, -0.3176],\n",
       "         [ 0.0118,  0.0510,  0.0588,  ..., -0.4275, -0.3255, -0.3882],\n",
       "         ...,\n",
       "         [-0.2392, -0.2157, -0.2078,  ..., -0.1216, -0.1686, -0.2157],\n",
       "         [-0.2549, -0.2235, -0.2157,  ..., -0.1294, -0.1529, -0.1765],\n",
       "         [-0.3020, -0.2235, -0.2000,  ..., -0.1451, -0.1843, -0.1843]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " X_train[78]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " CIFAR_train_list = [  ( X_train[i],  y_train[i].item() )  for i in range( X_train.shape[0]   )  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " CIFAR_test_list = [  ( X_test[i],  y_test[i].item() )  for i in range( X_test.shape[0]   )  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64  ## 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = torch.utils.data.DataLoader( CIFAR_train_list, batch_size=batch_size, shuffle=True  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dl = torch.utils.data.DataLoader( CIFAR_test_list, batch_size=10000, shuffle=True  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Residual Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ResNet\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(MyResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "32*32*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DL_3h_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear( 32*32*3 , 200)\n",
    "        self.act1    = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(200 , 100)\n",
    "        self.act2   = nn.ReLU()\n",
    "        \n",
    "        self.linear3 = nn.Linear( 100 ,50)\n",
    "        self.act3    = nn.ReLU()\n",
    "        \n",
    "        self.linear4 = nn.Linear(50 , 10)\n",
    "        self.act4    = nn.Softmax(dim=1)\n",
    "        \n",
    "        ## self.norm    = nn.LayerNorm()\n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x            = self.linear1(x)\n",
    "        x            = self.act1(x)\n",
    "        x            = self.linear2(x)\n",
    "        x            = self.act2(x)\n",
    "        x            = self.linear3(x)\n",
    "        x            = self.act3(x)\n",
    "      \n",
    "        x            = self.linear4(x)\n",
    "        y_pred       = self.act4(x)\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 128 , 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax(dim=1)        ## nn.LogSoftmax()\n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y_pred = self.model( x  )\n",
    "       \n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        # self.shape = shape,\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "class CNN_net_DH(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            ## nn.Conv2d(32, 32, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            View((-1, 256)),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(64, 10) # Output 10 classes\n",
    "            ## nn.Softmax(dim=1)        ## nn.LogSoftmax()\n",
    "        )\n",
    "        \n",
    "            \n",
    "     \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y_pred = self.model( x  )\n",
    "       \n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear( 32*32*3 ,20)\n",
    "        self.act1    = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(20 , 10)\n",
    "        self.act2    = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.norm    = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x            = self.linear1(x)\n",
    "        x            = self.act1(x)\n",
    "        x            = self.norm(x)\n",
    "        x            = self.dropout(x)\n",
    "        \n",
    "        x            = self.linear2(x)\n",
    "        y_pred       = self.act2(x)\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop( N_Epochs, model, loss_fn, opt ):\n",
    "    for epoch in range(N_Epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            ## xb = xb.view(  (16, -1 ) )\n",
    "            \n",
    "            xb = xb.to( torch_device )\n",
    "            yb = yb.to( torch_device )\n",
    "            \n",
    "            y_pred = model(xb)\n",
    "            \n",
    "            loss = loss_fn(y_pred, yb)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"loss=\", loss)\n",
    "            new_PATH = PATH + str(epoch)\n",
    "            print( new_PATH )\n",
    "            torch.save(model, new_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## torch.save(model, PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Core functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_Epochs      = 500\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## model = MLP_net()\n",
    "## model =  DL_3h_net()\n",
    "\n",
    "## model = CNN_net()\n",
    "\n",
    "## model = CNN_net_DH()\n",
    "## model.to( torch_device )\n",
    "\n",
    "model = MyResNet(ResidualBlock, [2, 2, 2]).to( torch_device )\n",
    "\n",
    "\n",
    "opt = torch.optim.SGD(  model.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.001 )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR100\n",
      "5 loss= tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR105\n",
      "10 loss= tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1010\n",
      "15 loss= tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1015\n",
      "20 loss= tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1020\n",
      "25 loss= tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1025\n",
      "30 loss= tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1030\n",
      "35 loss= tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1035\n",
      "40 loss= tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1040\n",
      "45 loss= tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1045\n",
      "50 loss= tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1050\n",
      "55 loss= tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1055\n",
      "60 loss= tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1060\n",
      "65 loss= tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1065\n",
      "70 loss= tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1070\n",
      "75 loss= tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1075\n",
      "80 loss= tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1080\n",
      "85 loss= tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1085\n",
      "90 loss= tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1090\n",
      "95 loss= tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1095\n",
      "100 loss= tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10100\n",
      "105 loss= tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10105\n",
      "110 loss= tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10110\n",
      "115 loss= tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10115\n",
      "120 loss= tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10120\n",
      "125 loss= tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10125\n",
      "130 loss= tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10130\n",
      "135 loss= tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10135\n",
      "140 loss= tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10140\n",
      "145 loss= tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10145\n",
      "150 loss= tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10150\n",
      "155 loss= tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10155\n",
      "160 loss= tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10160\n",
      "165 loss= tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10165\n",
      "170 loss= tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10170\n",
      "175 loss= tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10175\n",
      "180 loss= tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10180\n",
      "185 loss= tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10185\n",
      "190 loss= tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10190\n",
      "195 loss= tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10195\n",
      "200 loss= tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10200\n",
      "205 loss= tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10205\n",
      "210 loss= tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10210\n",
      "215 loss= tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10215\n",
      "220 loss= tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10220\n",
      "225 loss= tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10225\n",
      "230 loss= tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10230\n",
      "235 loss= tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10235\n",
      "240 loss= tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10240\n",
      "245 loss= tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10245\n",
      "250 loss= tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10250\n",
      "255 loss= tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10255\n",
      "260 loss= tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10260\n",
      "265 loss= tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10265\n",
      "270 loss= tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10270\n",
      "275 loss= tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10275\n",
      "280 loss= tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10280\n",
      "285 loss= tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10285\n",
      "290 loss= tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10290\n",
      "295 loss= tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10295\n",
      "300 loss= tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10300\n",
      "305 loss= tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10305\n",
      "310 loss= tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10310\n",
      "315 loss= tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10315\n",
      "320 loss= tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10320\n",
      "325 loss= tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10325\n",
      "330 loss= tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10330\n",
      "335 loss= tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10335\n",
      "340 loss= tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10340\n",
      "345 loss= tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10345\n",
      "350 loss= tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10350\n",
      "355 loss= tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 loss= tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10360\n",
      "365 loss= tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10365\n",
      "370 loss= tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10370\n",
      "375 loss= tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10375\n",
      "380 loss= tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10380\n",
      "385 loss= tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10385\n",
      "390 loss= tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10390\n",
      "395 loss= tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10395\n",
      "400 loss= tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10400\n",
      "405 loss= tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10405\n",
      "410 loss= tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10410\n",
      "415 loss= tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10415\n",
      "420 loss= tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10420\n",
      "425 loss= tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10425\n",
      "430 loss= tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10430\n",
      "435 loss= tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10435\n",
      "440 loss= tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10440\n",
      "445 loss= tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10445\n",
      "450 loss= tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10450\n",
      "455 loss= tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10455\n",
      "460 loss= tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10460\n",
      "465 loss= tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10465\n",
      "470 loss= tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10470\n",
      "475 loss= tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10475\n",
      "480 loss= tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10480\n",
      "485 loss= tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10485\n",
      "490 loss= tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10490\n",
      "495 loss= tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR10495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_loop( N_Epochs, model, loss_fn, opt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23\n",
      "Confusion Matrix:\n",
      "[[619  53   9   4   5  12  16  33  44 205]\n",
      " [  3   1  80  17 859  15   4   5   4  12]\n",
      " [  3   2  17 892  16  55   5   0   2   8]\n",
      " [ 47 787  18   2   6  11   8  22  65  34]\n",
      " [ 37  45  18   7  11   5  57  60 696  64]\n",
      " [  1   6  27  56  25 866   3   6   3   7]\n",
      " [ 62  22  75   5   9   4  80 621  58  64]\n",
      " [ 24   5   9   8  11   3 801  38  30  71]\n",
      " [  6  10 839  13  37  23   8  36  13  15]\n",
      " [133  39  24   6   8  14  58  48  53 617]]\n",
      "Precision: 0.225\n",
      "Recall: 0.226\n",
      "F1-measure: 0.225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for x_real, y_real in test_dl:\n",
    "        \n",
    "        batch_size = x_real.shape[0]\n",
    "        \n",
    "        ## x_real = x_real.view(  (batch_size, -1 ) )\n",
    "        \n",
    "        x_real = x_real.to( torch_device )\n",
    "        \n",
    "        y_pred = model(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function( y_real, preds.cpu() )\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## From checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model class must be defined somewhere\n",
    "model2 = torch.load('/scratch/scholar/rcalix/CNN_model_CIFAR1095')\n",
    "## model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22\n",
      "Confusion Matrix:\n",
      "[[618  61   8   4   5   5  26  39  42 192]\n",
      " [  6   2  57  25 863  17   5  16   3   6]\n",
      " [  5   1  12 874  18  68   7   2   7   6]\n",
      " [ 38 816  19   0   3  14   5  21  50  34]\n",
      " [ 46  66  15   0  14   2  63  52 682  60]\n",
      " [  2   5  22  65  28 849   8   8   5   8]\n",
      " [ 77  35  64   3  14   5  60 593  86  63]\n",
      " [ 20   9   9   6   9   3 776  46  53  69]\n",
      " [  5  10 789  17  59  27   9  55  12  17]\n",
      " [137  47  17   7  11  12  63  57  64 585]]\n",
      "Precision: 0.221\n",
      "Recall: 0.220\n",
      "F1-measure: 0.220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for x_real, y_real in test_dl:\n",
    "        \n",
    "        batch_size = x_real.shape[0]\n",
    "        \n",
    "        ## x_real = x_real.view(  (batch_size, -1 ) )\n",
    "        \n",
    "        x_real = x_real.to( torch_device )\n",
    "        \n",
    "        y_pred = model2(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(y_real, preds.cpu() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rc = data_train[4].view((-1))\n",
    "rc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rc = torch.unsqueeze(rc, dim=0)\n",
    "rc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## example_label = model( rc )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## example_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Figure out the dimensions of CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_batches_rc = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "model_rc = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten()\n",
    "            \n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "model_rc  = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 3\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_tensor_test   = torch.randn(N_batches_rc, 3, 32,  32)\n",
    "\n",
    "res_actual_model = model_rc(  my_tensor_test   )\n",
    "\n",
    "res_actual_model.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Anaconda 2020.02)",
   "language": "python",
   "name": "anaconda-2020.02-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
