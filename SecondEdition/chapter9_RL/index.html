<html>
<head>

  <link href="style.css" rel="stylesheet" type="text/css" />
</head>

  <body>

<div class="navbar">
  <a href="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/index.html"> Deep Learning </a>
  <a href="https://ricardocalix.substack.com">Substack</a>
  <a href="https://www.youtube.com/channel/UCKRgi-HJDEq0a3nhlG2nQvg">YouTube</a>
  <a href="https://github.com/rcalix1/DeepLearningAlgorithms/tree/main/SecondEdition">GitHub</a>
  <a href="https://www.galacticbackwater.com/theAIhub/index.html">Recommender</a>
  <a href="https://amzn.to/3OauEG0">Books</a>
  <a href="https://www.linkedin.com/in/ricardo-calix-phd">About</a>
  <a href="https://scholar.google.com/citations?hl=en&user=TiKVs6AAAAAJ">Scholar</a>	
  <a href="">Shop</a>
  <a href="https://www.rcalix.com">Contact</a>
</div>

    

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->

<div class="main">    <!-- for the fixed nav bar -->

<h1>Chapter 9 - Reinforcement Learning</h1>

    <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/frozen_lake.png" height="400" width="auto">
      </div>

    </center>

<p>

In this section of the book I will cover the topic of Reinforcement Learning. This is an area of machine learning somewhere between supervised learning and unsupervised 
	learning. It has been extensively applied to recommender systems and AI-based games. Recently, it was shown that a deep Q-network, using only pixels and game scores
	as inputs, could achieve a playing level comparable to that of professional human gamers across a set of 49 Atari games (\babelEN{\cite{mnihRef}}). The main advantage 
	of applying reinforcement learning to games is that games are governed by rules. You have game states (the inputs) and actions (output) that lead to new states and 
	rewards (the objectives to maximize). Because of this, no annotation is needed and instead you rely on the rules of the game for feedback (e.g. instead of annotated 
	labels). 
	
</p>




<h1>Copyright, License, FTC and Amazon Disclaimer</h1>

<p>
 Copyright &copy by Ricardo A. Calix. <br/>
 All rights reserved. No part of this work may be reproduced or transmitted in any form or by any means, without written permission of the copyright owner. <br/>
 This post/page/article includes Amazon Affiliate links to products. This site receives income if you purchase through these links. 
 This income helps support content such as this one. 
 <br/>

	

  
</p>

     <center>
      <div class="img"> 
        <a href="https://amzn.to/3vOL8NF"><img src="https://m.media-amazon.com/images/I/71Wi+z5fKzL._SL1233_.jpg" height="500" width="auto"></a>
      </div>

    </center>
    




<h1>

Reinforcement Learning
	
</h1>


<p>


	In this section of the book I will cover the topic of Reinforcement Learning. This is an area of machine learning somewhere between supervised learning and unsupervised 
	learning. It has been extensively applied to recommender systems and AI-based games. Recently, it was shown that a deep Q-network, using only pixels and game scores
	as inputs, could achieve a playing level comparable to that of professional human gamers across a set of 49 Atari games (\babelEN{\cite{mnihRef}}). The main advantage 
	of applying reinforcement learning to games is that games are governed by rules. You have game states (the inputs) and actions (output) that lead to new states and 
	rewards (the objectives to maximize). Because of this, no annotation is needed and instead you rely on the rules of the game for feedback (e.g. instead of annotated 
	labels). 
There are several types of reinforcement learning techniques. In this chapter, I will focus on getting started with Q-learning since this is the technique used in 
	the Mnih et al (2015) paper I referenced above. Here, I will try to provide a simple intuition based description of the technique. I should note that to achieve 
	the level of Q-Learning presented in the Mnih et al (2015) paper, several additional optimizations need to be included. However, the discussion in this chapter 
	should provide a simple way to get started with Q-Learning. 
</p>



<h1>

 So what is Q-Learning?
	
</h1>

<p>

Q-Learning tries to learn the value of being in a given state (s), and taking a specific action from there. 
As I indicated, Q-learn has been applied to games. The best way to understand the algorithm is to analyze it from the point of view of a game. Here we will use 
	Python’s OpenAI Gym module to play games. We will select the simple FrozenLake game environment. 
FrozenLake is a game about crossing a frozen lake that has some cracks in the ice with holes and there is wind sometimes that pushes the person crossing it. 
	The game is very simple and consists of a grid that is 4x4 like so. 
	
</p>




 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/frozen_lake.png" height="400" width="auto">
      </div>

    </center>



<p>
So, the objective is to get to the cheese without falling into a hole or being pushed by the wind into a hole. There are 4 moves which are up, down, right, and 
	left. There is only one reward and that is to get to the cheese. However, you only get that reward in the future by first taking several steps on frozen 
	blocks without falling in a hole. Therefore, one challenge is that you have to state your objective in terms of several future moves. This is accomplished
	using something called the Bellman Equation. 




The key to predicting these rewards is to know the associated reward given a current state and action to take. This is called a Q mapping 
	
</p>


<center>


	<p>

		    Q (state, action) = reward
	</p>
</center>


	<p>

                
For such a simple grid, we could just use a table. In this case our table would be 16x4 because there are 16 possible states (position in the grid of 4x4) and there 
	are 4 actions (up, down, right, left). Since we know the rules of the game and the layout of the grid, we can populate the table and learn the Q rewards for
	each state/action pair. 
An example of the table can be seen below.
		
	</p>

                





 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/q_learn_table.png" height="400" width="auto">
      </div>

    </center>


<p>
Now the main challenge is that we need to learn future rewards for future actions as we move through the grid. Here the Bellman equation will help. 
	Think of the Bellman equation as a type of recursive equation that looks at the future state given a current state. The Bellman equation is as follows:

	
</p>	


<center>
 Q(state, action) = reward + weight * max [ Q(future_state, future_action ) ] 

	
</center>
   
<p>
These values can be looked up from the Table. 
The code discussed here can be downloaded from the course website or the github repository. In the next section, the python Q-learning code will be 
	discussed which only uses a table to determine the rewards and the path to follow. Future sections will use the same algorithm but will replace the use of
	the table with a neural network so that we can see how deep neural networks can improve the approach. 

	
</p>
  
<h1>
Q-Learning using a Table
	
</h1>

<p>
In this section we discuss the code to implement Q-Learning using a table. This code makes use of the OpenAI gym library.  
	The libraries used can be seen in the next code segment. 

	
</p>


<center>
<div>
<textarea rows="4" cols="90">

import numpy as np
import gym

  
</textarea>
  
</div>
</center>
	










The frozenLake game can be initialized by creating the env object as can be seen below. This object represents the game and holds all the parameters related to states, actions, rewards, and current game state. 

<center>
<div>
<textarea rows="4" cols="90">

env = gym.make('FrozenLake-v0')


  
</textarea>
  
</div>
</center>
	







The next step is to initialize the table $ Q $ to all zeros and of size 16x4. Here env.observation\_space.n = 16 and env.action\_space.n = 4.

<center>
<div>
<textarea rows="7" cols="90">

Q = np.zeros( [env.observation_space.n, env.action_space.n] )

lr           = 0.8
y            = 0.95
num_episodes = 2000

  
</textarea>
  
</div>
</center>
	
	









We take 2000 epochs (or episodes) and initialize some parameters lr and y. Each episode represents a game played. We use \textbf{jList} and \textbf{rList} to collect the number of steps taken per episode and the total reward per episode, respectively.  These are used to collect results of each game. 



<center>
<div>
<textarea rows="5" cols="90">


jList = []
rList = []
  
</textarea>
  
</div>
</center>









The following code segment goes over the main loop of the Q-learn algorithm. 



<center>
<div>
<textarea rows="20" cols="90">

for i in range(num_episodes):
    s = env.reset()
    rAll = 0
    d = False j=0
    while j < 99:
        j+=1
        zz = env.action_space.n 
        a=np.argmax(Q[s,:]+np.random.randn(1,zz) *(1.0/(i+1))) 
        s1,r,d,_ = env.step(a)
        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a]) 
        rAll += r
        s = s1
        if d == True:
            break
    #jList.append(j)
    rList.append(rAll)

  
</textarea>
  
</div>
</center>






                


In the next code segment, the line



<center>
<div>
<textarea rows="5" cols="90">

for i in range(num\_episodes):

  
</textarea>
  
</div>
</center>
	







                 
indicates that we are going to play num\_episodes = 2000 games. During these 2000 tries we will learn the best path to take.  


From the main loop we can see that the line 

\begin{center}
   s = env.reset() 
\end{center}


                                   
restarts the game for every episode so we can play it again and assign the initial state to \textbf{s}. The variable \textbf{rAll} adds up the accumulated rewards for this episode. The variables \textbf{d} and \textbf{j} are control variables to indicate if the game has ended and to count the number of steps taken.  


The code in the while loop is what allows the algorithm to learn or update the values in the \textbf{Q} table designated by the variable \textbf{Q}. To take the first step we need to pick an action to follow. We do this with the following lines of code:

\begin{center}
    zz = env.action\_space.n 
\end{center}



and

\begin{center}
a = np.argmax(  Q[s,:]+np.random.randn(1,zz) *(1.0/(i+1))  ) 
\end{center}
              
                                  

The variable \textbf{zz} is the size \textbf{n} of all actions in the game (up, down, left, right) which in this case is 4. The statement Q[s, :] selects the current Q values (rewards) associated with state \textbf{s}. The statement 

\begin{center}
     np.random.randn(1,zz) *(1.0/(i+1))  
\end{center}
           

adds randomness to the four Q values for the current state. Basically, you randomly increment the Q values for the current state and then select the highest one with 

\begin{center}
      np.argmax()
\end{center}
                       
                                        
by selecting the highest Q value you determine what action (a) you take given the current state. 


Once the action \textbf{a} is selected, we can proceed to evaluate it in the game to obtain our new state (position) and the reward (did we fall in a hole or advanced to a frozen block). We do this with 


\begin{center}
      s1, r, d, \_ = env.step(a)
\end{center}

                    

here, \textbf{s1} is the new state (position) and \textbf{r} is the reward. The parameter \textbf{d} indicates end of the game. Given this new information about the result of our action, we can proceed to update the Q-table with our new results and new knowledge about the state of the game. This is done with the statement 

\begin{center}
         Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])
\end{center}

 
    
In this statement, \textbf{Q[s, a]} contains the current Q value (reward) associated with the state \textbf{s} and the action \textbf{a}. This is the Bellman equation which can be viewed as

\begin{center}

         next\_s\_Q = lr*(r + y*np.max(Q[s1,:]) - Q[s,a])

         Q[s,a] = Q[s,a] + next\_s\_Q
         
\end{center}


The term next\_s\_Q contains the current reward for state \textbf{s} plus the maximum reward for the next state \textbf{s1}.   The parameters \textbf{lr} and \textbf{y} are weights to control the importance of the next state’s reward when updating the current states reward (Q value). 
We can think of this parameter 


\begin{center}

         - Q[s,a] )
         
\end{center}
                            
                                                 
as a regularization parameter. 
At this point we are almost done and we can proceed to accumulate our results. The statement 

\begin{center}

         rAll += r
         
\end{center}

   
accumulates the total rewards. The statement

\begin{center}

         s = s1
         
\end{center}

          
          
assigns the current state \textbf{s1} to \textbf{s}. 

The "if" statement in the folowing code listing


<center>
<div>
<textarea rows="5" cols="90">

if d == True:
    break
  
</textarea>
  
</div>
</center>
	






            
        
ends the game if \textbf{d} indicates end of game. The statement

\begin{center}

        jList.append(j)
         
\end{center}

                  

accumulates the number of steps taken to reach end of game. The statement


<center>
<div>
<textarea rows="5" cols="90">

  rList.append(rAll)
  
</textarea>
  
</div>
</center>
     
         



                  
                   
appends rewards per game to a list so that they can be viewed later. 


That is it. We have finished our discussion of Q-learn with tables on the frozenLake game. Now we can proceed to replace the table with a neural network.


\section{Q-Learning using a Neural Network}  

   
Now that we understand the frozenLake game with a table, we can proceed to replace the table with a neural network. It is important to note here that the weights matrix \textbf{W} in the neural network can be thought of as  representing the \textbf{Q} table. Otherwise, just think of the whole neural network as the table. We give it the states vector (size=16) as input and the network predicts the actions vector (size=4) per state.  
Let us begin. 

First we include the libraries as can be seen below. Notice we now add PyTorch.


<center>
<div>
<textarea rows="10" cols="90">

import gym
import numpy as np
import random
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

  
</textarea>
  
</div>
</center>
	







We create the game with the env object. 



<center>
<div>
<textarea rows="10" cols="90">


env = gym.make('FrozenLake-v1')
  
</textarea>
  
</div>
</center>








Next, we define our familiar neural network classes and functions for the  loss and training. The first Q NN class creates a simple logistic regression type neural network. Here, the \textbf{W} replaces the \textbf{Q} table. Notice the dimensions of \textbf{W} are 16x4 because we have 16 states in the game and 4 actions. The \textbf{Qout} tensor (our predicted \textbf{y} in previous chapters) is the result of a matmul operation between \textbf{x} (our states) and \textbf{W} (the weights or Q values in this case). 




<center>
<div>
<textarea rows="10" cols="90">


class Q_NN_Net(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.linear1    = nn.Linear(16, 4)
        self.act1       = nn.Softmax(dim=1)

    def forward(self, s):
        
        x          = gen_state_vector(s)
        
        x          = self.linear1( x )
        Qout       = self.act1( x )
        
        Qout       = torch.squeeze(Qout, 0)

        return Qout
  
</textarea>
  
</div>
</center>
	







With 

\begin{center}
      torch.squeeze(Qout, 0)
\end{center}


we get the actions vector. We will use a torch.max function later to select the action. 


As can be seen from the previous class code, the network looks like the figure below.  

  <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/rl_nn.png" height="400" width="auto">
      </div>
    </center>



It is important to note that this is a basic architecture and that much more complex deep architectures with different activation functions could be used such as architectures with many hidden layers or convolutional neural networks, etc. I was able to achieve good results using the following MLP neural network architecture.


<center>
<div>
<textarea rows="10" cols="90">


class Q_NN_MLP_Net(nn.Module):
    
    def __init__(self):
        
        super().__init__()
        
        self.linear1    = nn.Linear(16, 10)
        self.act1       = nn.ReLU()        ## Tanh()  ## nn.Sigmoid()
        self.linear2    = nn.Linear(10, 4)
        self.act2       = nn.Softmax(dim=1)

    def forward(self, s):
        
        x    = gen_state_vector(s)
        
        x    = self.linear1( x )
        x    = self.act1(    x )
        x    = self.linear2( x )
        Qout = self.act2(    x )
        
        Qout = torch.squeeze(Qout, 0)
        
        return Qout
    

</textarea>
  
</div>
</center>
	



	




Notice that the NN classes us the following function called \textbf{gen\_state\_vector(s)}. We will use this function to convert state in integer form into one hot encoded vectors of size 16. 

<center>
<div>
<textarea rows="10" cols="90">


def gen_state_vector(s):
    states_np = np.identity(16)[s:s+1]
    states_np = states_np.astype(   np.float32  )
    ## print(states_np.dtype )
    inputs1 = torch.from_numpy( states_np )
    return inputs1
    

</textarea>
  
</div>
</center>
	



	



    

The function

\begin{center}

    gen\_state\_vector(s)
    
\end{center}

            
takes the current state in the variable \textbf{s} and converts it into a one-hot encoded representation. For instance, if the current state is 4, then the one-hot encoded representation (of size 16) looks like this

\begin{center}
     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
\end{center}

The loss function can be Least Squares Estimation which is the same as for regression! To calculate the error, we basically compare \textbf{q\_pred} to \textbf{target\_q} and try to minimize the error. 



<center>
<div>
<textarea rows="6" cols="90">

loss = F.smooth_l1_loss( q_pred, target_q )

## or

loss_fn = nn.MSELoss()
    

</textarea>
  
</div>
</center>
	







We can set the following parameters. We create lists to contain  steps taken per episode (game) and  rewards per game. 



<center>
<div>
<textarea rows="9" cols="90">

stepsList     = []
rewardsList   = []
success_list  = [0.0005]
learning_rate = 0.001       ## Adam default learning rate
y             = 0.99
num_episodes  = 4000
epsilon       = 0.1       ## 0.2    , 0.01
    

</textarea>
</div>
</center>








The optimization is nothing more than the very familiar Gradient Descent (Adam) with a learning rate of 0.001. We instantiate the Q NN MLP model.




<center>
<div>
<textarea rows="7" cols="90">

## model = Q_NN_Net()

model = Q_NN_MLP_Net()

opt = torch.optim.Adam( model.parameters(), lr=learning_rate )


</textarea>
  
</div>
</center>








Finally, we are ready for the main loop which is shown in the next code segment below. 


<center>
<div>
<textarea rows="25" cols="90">



for i in range(num_episodes):
    
    s = env.reset()
    s    = s[0]  ## env returns tuple so select first of tuple
    rAll = 0  
    d    = False 
    j = 0
    
    if i % 200 == 0:
         print("Game ", i, " of ", num_episodes)
    
    while j < 1000:
        
        Q_s          = model(  s  ).detach()
        _, max_index = torch.max(Q_s, 0) 
        a            = max_index.item()
             
        s1, r, d, _, _ = env.step(a)
              
        if d == True and r == 0: 
             r = -1
        
        Q_s1      = model(  s1 ).detach()     # detach from graph
        maxQ_s1   = torch.max( Q_s1 )  
        target_q  = r + 0.99 * maxQ_s1
        
        q_pred = model(  s  )[a]   
        ## loss = loss_fn(  q_pred, target_q )
        loss = F.smooth_l1_loss( q_pred, target_q )   
        opt.zero_grad()
        loss.backward()
        opt.step()
        
        rAll = rAll + r
        j = j + 1
        s = s1
        if d == True: 
            break
           
    if d == True and r > 0:
        success_list.append(1)     
        stepsList.append(j)          ##  steps taken per game
    else:
        success_list.append(0)
                
           
    ## stepsList.append(j)           ##  steps taken per game
    rewardsList.append(rAll)         ## reward total per game
    



</textarea>
  
</div>
</center>
	




I will now proceed to break the main loop into parts to make it easier to describe. 
As can be seen in the code segment below, we run the main loop 4000 times (\textbf{num\_episodes}) which means that we play 4000 games. Each time we play a game, we reinitialize the board ( s = env.reset() ) and initialize the rewards variable (\textbf{rAll}) to zero. The variable \textbf{j} is the counter for the current step and \textbf{d} is used to determine if the game is over (win or loss). 


<center>
<div>
<textarea rows="10" cols="90">


for i in range(num_episodes):
    
    s = env.reset()
    s    = s[0]  ## env returns tuple so select first of tuple
    rAll = 0  
    d    = False 
    j = 0
    
    if i % 200 == 0:
         print("Game ", i, " of ", num_episodes)



</textarea>
</div>
</center>

	






        
for every game iteration we run the following while loop. This while loop is the main code that helps us to learn the \textbf{Q} values and traverse the board (e.g. play the frozen lake game). 


<center>
<div>
<textarea rows="20" cols="90">



while j < 1000:
        
        Q_s          = model(  s  ).detach()
        _, max_index = torch.max(Q_s, 0) 
        a            = max_index.item()
            
        s1, r, d, _, _ = env.step(a)
              
        if d == True and r == 0: 
             r = -1
        
        Q_s1      = model(  s1 ).detach()     # detach from graph
        maxQ_s1   = torch.max( Q_s1 )  
        target_q  = r + 0.99 * maxQ_s1
        
        q_pred = model(  s  )[a]   
        ## loss = loss_fn(  q_pred, target_q )
        loss = F.smooth_l1_loss( q_pred, target_q )
                
        opt.zero_grad()
        loss.backward()
        opt.step()
        
        rAll = rAll + r
        j = j + 1
      
        s = s1
        if d == True: 
            break

                


</textarea>
</div>
</center>
	




   
        
We perform 1000 steps since it should not take more than 1000 steps to traverse the frozen lake. If it does, the game should end. The line in the while loop is used to increment the steps 

\begin{center}
     j = j + 1
\end{center}
           
            
       
After incrementing the steps, we proceed to perform our first model run to train the PyTorch model. Here we predict an action and get Q\_s from the model. 


<center>
<div>
<textarea rows="7" cols="90">

Q_s          = model(  s  ).detach()
_, max_index = torch.max(Q_s, 0) 
a            = max_index.item()
   

</textarea>
</div>
</center>
	


	





                    
The next step is to take the predicted action in \textbf{“a”} and run it through the game. The below statement runs the action through env.step() and this function returns the new state \textbf{s1} which is the new position in the frozen lake grid, \textbf{r} is the reward associated with the step \textbf{s} (for instance r = 0.43) , and \textbf{d} indicates if the game is over (found the cheese or fell in the frozen lake). You know you have lost the game if \textbf{d} is True and if the reward is zero at the same time. To help the model learn, we can assign negative rewards every time that happens. Knowing when to assign rewards is very challenging in Reinforcement Learning. Here, frozen lake is a simple game and using this type of approach is possible. Current research proposes to replace the rewards heuristics with neural networks. One current effective approach is called RLHF which stands for reinforcement learning through human feed-backs.  


<center>
<div>
<textarea rows="7" cols="90">

s1, r, d, _, _ = env.step(a)
              
if d == True and r == 0: 
    r = -1
     
  

</textarea>
</div>
</center>
	



              


   

With the new state \textbf{s1}, we proceed to run the PyTorch model again.  Here we run the model again using \textbf{s1}. 

<center>
<div>
<textarea rows="7" cols="90">

Q_s1      = model(  s1 ).detach()     # detach from graph
_, maxQ_s1   = torch.max( Q_s1 )  
target_q  = r + 0.99 * maxQ_s1   
  

</textarea>
</div>
</center>
	







               



So \textbf{Q\_s1} will now contain the 4 neuron vector with the \textbf{Q} values for all 4 actions given state \textbf{s1}. We grab the max value from \textbf{Q\_s1} (not the index) and use that to calculate \textbf{target\_q} using the Bellman equation.

Recall that the Bellman equation looks like this:

\begin{center}
Q(state, action) = reward + weight * max [ Q(future\_state, future\_action )] 
\end{center}


           
Interestingly, only one of the 4 values in \textbf{Q\_s1} is updated using the Bellman equation. The other values remain the same. The dimension of \textbf{target\_q} is only one which means that that is the only value we will use to perform the optimization.


Finally, we do a final update of the PyTorch model by calculating the losses on state \textbf{“s”} and action \textbf{a}.  


<center>
<div>
<textarea rows="7" cols="90">

q_pred = model(  s  )[a]   
      
## loss = loss_fn(  q_pred, target_q )
loss = F.smooth_l1_loss( q_pred, target_q )
                
opt.zero_grad()
loss.backward()
opt.step()  
  

</textarea>
</div>
</center>






               
           




Finally, the last peace of code adds up the rewards, assigns the new state \textbf{s1} to \textbf{s}, and checks to see if the game is over. 


<center>
<div>
<textarea rows="7" cols="90">

rAll = rAll + r
j = j + 1
      
s = s1
if d == True: 
    break
  

</textarea>
</div>
</center>

	

           

Once you exit the while loop, the last part is to append the results of the current game to the results lists.  


<center>
<div>
<textarea rows="7" cols="90">

if d == True and r > 0:
    success_list.append(1)     
    stepsList.append(j)          ##  steps taken per game
else:
    success_list.append(0)
                
           
## stepsList.append(j)           ##  steps taken per game
rewardsList.append(rAll)         ## reward total per game
 
  

</textarea>
</div>
</center>
	


              


      

Well, that is it for the algorithm discussion. Finally, we print our results and plot them. 


<center>
<div>
<textarea rows="10" cols="90">

print("Percent of succesful episodes: " ,
                      str( sum(rewardsList)/ num_episodes ),  "%")
plt.plot(rewardsList)
plt.show()
plt.plot(stepsList)
plt.show()

</textarea>
</div>
</center>
	




               



That is it. We have completed implementing our Q learning algorithm with a neural network. In the next section we will add a simple improvement to the code that will improve performance.

\section{Performance, Q-NN, and randomness }

In the previous section we described the code to implement Q-learning with a neural network on the frozen lake game. That was the simplest implementation of it. 


<center>
<div>
<textarea rows="25" cols="90">

for i in range(num_episodes):
    s = env.reset()
    s    = s[0]  ## env returns tuple so select first of tuple
    rAll = 0  
    d    = False 
    j = 0
    if i % 200 == 0:
         print("Game ", i, " of ", num_episodes)
    while j < 1000:
        if (np.random.rand(1) < epsilon): 
            a = env.action_space.sample()
        else:
            Q_s          = model(  s  ).detach()
            _, max_index = torch.max(Q_s, 0) 
            a            = max_index.item()
        s1, r, d, _, _ = env.step(a)  
        if d == True and r == 0: 
             r = -1
        Q_s1      = model(  s1 ).detach()     # detach from graph
        maxQ_s1   = torch.max( Q_s1 )  
        target_q  = r + 0.99 * maxQ_s1
        q_pred = model(  s  )[a]   
        ## loss = loss_fn(  q_pred, target_q )
        loss = F.smooth_l1_loss( q_pred, target_q )    
        opt.zero_grad()
        loss.backward()
        opt.step()
        rAll = rAll + r
        j = j + 1
        s = s1
        if d == True: 
            break          
    epsilon = epsilon + epsilon_delta    
    if d == True and r > 0:
        success_list.append(1)     
        stepsList.append(j)          ##  steps taken per game
    else:
        success_list.append(0)            
    ## stepsList.append(j)           ##  steps taken per game
    rewardsList.append(rAll)         ## reward total per game
    


</textarea>
</div>
</center>
	





To improve the results, we can add a few lines of additional code which will allow the algorithm to better converge and learn better Q-values. The additions are simple and basically relate to adding randomness to the code. The full version of the code can be seen below. I will additionally discuss how to measure performance with RL.

The new lines of code add randomness to the selection of the next action to take. The idea is that at the beginning of the learning process, the action prediction function may not be very good. Therefore, picking an action randomly at the beginning may be better than picking actions with the model. 


This is reflected in the code segment below. 


<center>
<div>
<textarea rows="10" cols="90">

if (np.random.rand(1) < epsilon): 
    a = env.action_space.sample()
else:
    Q_s          = model(  s  ).detach()
    _, max_index = torch.max(Q_s, 0) 
    a            = max_index.item()

</textarea>
</div>
</center>







       
A random number is obtained and compared to \textbf{epsilon}. If less than \textbf{epsilon}, the action is selected randomly. As the algorithm improves and the Q values are better, the value of \textbf{epsilon} can be adjusted so that action is more often selected with the model and not with the random function.



   
The code can be seen here  where  the value of \textbf{“epsilon”} is adjusted.  



<center>
<div>
<textarea rows="5" cols="90">

epsilon = epsilon + epsilon_delta

</textarea>
</div>
</center>
	






     


That is all it takes to implement randomness. Now we are ready to measure the performance of the RL model using frozen lake. After training, we can perform several calculations to measure performance. 

First let us count the number of successes during the last 100 training games (out of 4000). The following calculation gives us a score of 75\%. 

<center>
<div>
<textarea rows="5" cols="90">

print("Succes for last 100 games: ", sum(success_list[-100:]),  "%")


</textarea>
</div>
</center>





     


We can look at the rewards trend with the following code. 

<center>
<div>
<textarea rows="25" cols="90">

## rewardsList = rewardsList[-20:]

step_size = 100

## new_avg_per_n_entries
r_rc = [  sum(rewardsList[i:i+step_size])/step_size for i in range(0, len(rewardsList), step_size )  ]

rewardsList = r_rc 

print(rewardsList)
print(set(rewardsList))
print(len(rewardsList))
print(sum(rewardsList))
print( num_episodes )


ind_rew = [i for i in range(len(rewardsList))]
plt.scatter(ind_rew, rewardsList)
## plt.plot(rewardsList)
plt.title("Rewards during training")
plt.show()

</textarea>
</div>
</center>




     


During training, we collected the total reward per each of the 4000 games. Visualising all 4000 rewards can be confusing. However, if we average every 100 rewards in sequence, we can clearly  see that the rewards are going up. One of the goals in RL is for the model to increase its rewards.  

\begin{figure}[H]\centering
\adjustbox{max height=.85\textheight}{
    \includegraphics{images/rewards_trend.png}
}
\caption{Rewards trend}
\label{RegLin:fig}
\end{figure}

Once the model is trained, we want to run it again without training to test its performance. This can be accomplished with the following code 

<center>
<div>
<textarea rows="5" cols="90">


stepsList    = [0.0005]
success_list = [0.0005]


test_games = 100

for i in range(test_games):
    s = env.reset()
    s = s[0]
    j = 0
    while j < 1000:
                    
        agent_out = model(s).detach()
        _, max_index = torch.max(agent_out, 0)   
        a = max_index.data.cpu().numpy()[()]


        s1, r, d, _, _ = env.step(a)
        if d == True and r == 0: 
            r = -1
                
        s = s1
        j = j + 1
            
        if d == True: 
            break
            
    if d == True and r > 0:
        success_list.append(1)
    else:
        success_list.append(0)
    stepsList.append(j)
    

</textarea>
</div>
</center>
	


     


After running the evaluation (no training), the model is able to win 79\% of the games it played. 

\begin{figure}[H]\centering
\adjustbox{max height=.85\textheight}{
    \includegraphics{images/games_won.png}
}
\caption{Games Won}
\label{RegLin:fig}
\end{figure}

<h1>
Summary
</h1>

<p>


In this chapter we have discussed the Q learning algorithm as part of the larger topic of Reinforcement Learning using tables and neural networks with randomization. 


	
</p>
















<p>
Each decoder layer consists of a decoder multi-head attention layer, followed by a fully connected layer. The attention layers consist of m_heads (e.g. 8) parallel 
	attention sub layers that are later concatenated. The numbers 6 and 8 are a choice the architect makes.

	
</p>


<h1>

	Teacher Forcing
</h1>
	  
<p>
You may have already read somewhere (on-line) that the Decoder in the Transformer network predicts one word at a time and that that word is read back as an input in the 
	next iteration. Also, the network predicts the last word in the sequence of words. But you may think, aren't those last words just padding? Eh? So, what is going on 
	here? As it turns out, the mechanism of predicting one word at a time and feeding it back as an input in the next iteration is only done during the "testing"
	phase and it is not done during "training". Instead, during "training" of a decoder we use “Teacher Forcing”.
Teacher forcing is a technique in auto regressive models where you do not use the predicted outputs of the decoder to feed back as input but instead you use the real data. 
	This helps the model to learn the correct information instead of its own erroneous predictions (especially at the beginning of training).
</p>

	  <h1>
Implementing a GPT in PyTorch from Scratch
		  
	  </h1>

	  <p>
In this section,  I will implement a simple GPT using everything we have learned in this book. The code will be very object oriented for efficiency. That being said, 
	this GPT is implemented from scratch, works really well, and can be scaled. Thanks to Andrej Karpathy for helping me to better understand the PyTorch 
		  implementation of a GPT (<a href="https://karpathy.ai">Andrej Karpathy</a>).

For the sake of simplicity I will not use subwords here and instead just use the letters of the English alphabet and a few symbols. The vocabulary of a large GPT such as
	GPT-4 could be hundreds of thousands of subwords or more. This GPT reads in one text file and trains on it. 

<br/>
Here, we first input our common python libraries.  

	
</p>


<center>
<div>
<textarea rows="8" cols="40">

import torch
import numpy as np
import torch.nn as nn

from torch.nn import functional as F
  
</textarea>
</div>
  </center>


<p>

	In the following code segment we can set the parameters as we have done before.

</p>
	


<center>
<div>
<textarea rows="20" cols="70">


torch.manual_seed(256)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

block_size        = 40      ## N tokens in sequence
batch_size        = 64 
max_iters         = 6000
eval_interval     = 500     
learning_rate     = 0.0003
eval_iters        = 300
vocab_size        = 65

## every id for a given token is embedded to vector of this size
n_embd            = 512                  
n_head            = 8         ## 8 attention heads
n_layer           = 6         ## 6 eoncoder layers
dropout           = 0.2
  
</textarea>
</div>
    </center>

      
<p>

Now we proceed to read the text data to train on. 

	
</p>




     <center>
<div>
<textarea rows="16" cols="100">


Wq = torch.tensor( [batch_size, 512, 64] )
bq = torch.tensor( [batch_size,  40, 64] )  
Q  = torch.matmul( x, Wq ) + bq    # Nx40x64
    
Wk = torch.tensor( [batch_size, 512, 64] )  
bk = torch.tensor( [batch_size,  40, 64] )  
K = torch.matmul(x, Wk) + bk    # Nx40x64
    
Wv = torch.tensor( xavier_init( [batch_size, 512, 64] )  )
bv = torch.tensor( torch.random_normal( [batch_size, 40, 64] )  )
V = torch.matmul(x, Wv) + bv    # Nx40x64


</textarea>
  
</div>
    </center>



<p>

Once Q, K, and V are defined, the next step is to multiply Q times K. You can think of this as calculating a score of the importance of "token_i" in
	"x" to all other words in "x".


	
</p>

 


  
     <center>
<div>
<textarea rows="17" cols="100">


## (B, T, 64) @ (B, E, 64)  -> (B, T, T)

wei = q @ k.transpose(-2, -1) * E ** -0.5        
        
wei = wei.masked_fill(
        self.tril[:T, :T] == 0, 
        float('-inf')
)   
        
## (B, T, T)
wei = F.softmax( wei, dim= -1 )         ## (B, T, T)
wei = self.dropout(   wei   )


</textarea>
  
</div>
    </center>

  

<p>

The variable "wei" is computed by a nn.matmul between Q and the transpose of K like so
<br/>
<br/>
    wei = nn.matmul( Q, K, transpose_b=True) 
<br/>
<br/>
	
This nn.matmul results in a matrix of size  [N, 40, 40]. We then divide "wei" by 
<br/>
<br/>
    sqrt(  Embd_size  )        
<br/>
<br/>
	
where Embd_size is equal to 64 (i.e. E ** -0.5). At this point "wei" continues to be of size [N, 40, 40].

The following code segment adds "wei" to the Mask. 


	
</p>






      <center>
<div>
<textarea rows="8" cols="80">

wei = wei.masked_fill(
        self.tril[:T, :T] == 0, 
        float('-inf')
)
  
</textarea>
  
</div>
</center>


<p>


The "look_ahead_mask" is of size [N, 40, 40]. This should be an addition of [N, 40, 40] + [N, 40, 40]. Notice that the wei.masked_fill function
	makes use on an infinity parameter. A simplified view of this operation is as follows:

<br/>
    wei = wei + (look_ahead_mask * -1e9)
<br/>


The final part of the forward function in the Head class (next code segment) finishes the Attention computation. The softmax is used to normalize on 
	the last axis so that the scores add up to 1 (axis -1 is for last dimension in the tensor). 


	
</p> 
        



  

<center>
<div>
<textarea rows="20" cols="100">


class Head(nn.Module):

    ...

    def forward(self, x):
        
        ...
        
        ## (B, T, T)
        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)
        wei = self.dropout(   wei   )
        
        ## perform weighted aggregation 
        
        v   = self.value(  x  )   ## (B, T, E)
        out = wei @ v             ## (B, 40, 40) @ (B, 40, 64) -> (B, 40, 64)
        
        return out

  
</textarea>
  
</div>
    </center>



<p>
   

Finally, the following operation is performed which result in a tensor of size [N, 40, 64]. Remember that 8 of these Head output tensors will be concatenated to return 
	to the original size of [N, 40, 512] (64 * 8 = 512). 

<br/>
out = nn.matmul(wei, V)    
<br/>
    
The operation looks like the following [N, 40, 40] * [N, 40, 64].


So, in summary you calculate the keys, queries, and values which are tensors that map the input \textbf{x} of size [N, 40, 512] to size [N, 40, 64]. We then
	calculate the scores matrix (wei) which is the Attention mechanism. This is a dot product. We matrix multiply Q with the transpose of K. This results 
	in a matrix that is size [N, 40, 40].

After calculating the score matrix (wei), we need to mask the values so that we don’t cheat by looking ahead. We apply the look ahead and padding masks. 
	The mask for look ahead attention happens before the softmax calculation. Notice that the masking is done to the dot_product scores matrix (wei) only. 
	The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is 
	applied immediately before a softmax. The goal is to zero out padded cells, and large negative inputs to softmax are near zero in the output.

<br/>
	<br/>
For example, the  softmax ( torch.nn.softmax(a7) ) for “a” defined as follows:
<br/>
	<br/>
a7 = torch.constant([0.6, 0.2, 0.3, 0.4, 0, 0, 0, 0, 0, 0]) 
 <br/>
	<br/>
gives the following

	
</p>
        

  
<center>
<div>
<textarea rows="8" cols="100">


<torch.Tensor: shape=(10,), dtype=float32, numpy=
array([0.15330984, 0.10276665, 0.11357471, 0.12551947, 0.08413821,
0.08413821, 0.08413821, 0.08413821, 0.08413821, 0.08413821], dtype=float32)>


</textarea>
</div>
</center>



<p>

now, if some of the values are negative infinities
<br/>
	<br/>
b7 = torch.constant([0.6, 0.2, 0.3, 0.4, -1e9, -1e9, -1e9, -1e9, -1e9, -1e9])
<br/>
	<br/>
then the softmax operation on b7 (torch.nn.softmax(b7)) should give us
<br/>

	
</p>



 


      <center>
<div>
<textarea rows="6" cols="100">

<torch.Tensor: shape=(10,), dtype=float32, numpy=
array([ 0.3096101 , 0.20753784, 0.22936477, 0.25348732, 0. ,0. , 0. , 0. , 0. , 0. ], dtype=float32)>

</textarea>
  
</div>
    </center>




 <p>


Notice the infinities are now zeros! 

<br/>
The decoder has a final linear layer after the 6 decoder_layer functions. The final layer in the decoder is the decoder_final_layer. This is a linear layer with 
	 no non-linearities and a softmax that maps the tensor [N, 40, 512] to a tensor of size [N, 40, n_vocab_size].

And that covers all the classes of the NN architecture. We are now ready to instantiate the GPT and call the core functions for the optimizer, etc. like so:

	 
 </p>


       <center>
<div>
<textarea rows="8" cols="100">


model   = GPTModel()

m       = model.to(device)

optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )


</textarea>
  
</div>
    </center>


<p>
The training function is presented below and is straight forward

	
</p>





        <center>
<div>
<textarea rows="18" cols="100">

for iter in range(max_iters):
    
    if iter % eval_interval == 0:
        losses = estimate_loss()
        print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

    xb, yb = get_batch('train')
    
    ## eval the loss
    logits, loss = m(xb, yb)
    
    optimizer.zero_grad(set_to_none=True)   ## zero out
    loss.backward()
    optimizer.step()
  

</textarea>
  
</div>
    </center>



<p>

Now, regenerate after some training


	
</p>





          <center>
<div>
<textarea rows="10" cols="100">

## Starting token  id_sos = 0
sos_context = torch.zeros(  (1, 1),  dtype=torch.long, device=device   )   

generated_text = m.generate(sos_context, max_new_tokens=500)[0].tolist()

print(  decode(generated_text)   )


</textarea>
  
</div>
    </center>




<p>
Using the generate function gives GPT generated text. 

Obviously, the amount of data will determine the quality of a GPT and it can take a lot of money and time to properly train a GPT. 
	The following are examples of generated text using just single books or scripts. 
<br/>
For South Park training data we can get
	
</p>



  
<center>
<div>
<textarea rows="20" cols="100">


Mr. Hankey: Who, don't look it, but looks like you have thrown them.
These chees. 
Stan: Now back a friend people.me. 
Kenny: (HA f!) 
Stan: The new bestsays to expon is weep thing the beny first they wan
t
Stan: Thank you.
Kyle: His realet! Stan, co musb me friends Vagisil magine?
Stan: Nat Just sind out hurt. Hallway gaves millions? Are Me: Preest
y revitalizing there?
Stan: D'RCRANNSSEA.
Stan: What?
Stan: Dammit!
Cartman: If anyone cools orget and have meet hundestly kids he since


  
</textarea>
  
</div>
    </center>



<p>

For Harry Potter training data we can get

	
</p>



 


<center>
<div>
<textarea rows="20" cols="100">


HARRY: What is saying that Time-Turner, ghe dangerous
and gentlemen —
GINNY: And is Albus Scorpius about to ldemborn King. And
we should be very moths die. He means the
rumble. It feels the appos his and forcement.
HARRY: Get out out of the train is and for the
Ministry had of fist or your pament.
DRACO: Wh, does it say?
HARRY: You came?
RON enters on a baccused by through through,
finality. It is it can’t better but Padma sets in for
one champions and to be to reface the Boy — she
speak.
HARRY: Year the rumors?
PROFESSOR McGONAGALL: I came it extraord the might be
minty Iave progice to your paplay — then your
nose? (She fining, an is hurts.) I, am Dad.
Sound that . . . weld your have beyou’ve gone?
SCORPIUS is saying in his roommount. What she
falls talking about the blanked than I had a son.
ALBUS: I’ll give to you that strave to do with my
died it son. And my for you because it fly. And
you schoolow how Harried in Potion
  

  
</textarea>
  
</div>
    </center>




<p>

And that concludes the discussion on GPTs.
	
</p>








	<h1>
Encoder Only Models
		
	</h1>

<p>
The encoder has 6 sub-layers called encoder layers. Each encoding layer has a Multi-Head Attention layer followed by a standard fully connected feed forward layer. 
	The input to the encoder goes through all these layers in the encoder and it is converted into an encoder output. The input to the encoder and the output of 
	the encoder have the same dimensions. For instance, here, the input to the encoder would be the English sentence. 

The attention layer consists of 8 parallel Attention sub layers that are later concatenated. The intuition is that each of these 8 layers can learn something new and 
	different. So this gives more capacity to the network. The input to the encoder goes through all these layers in the encoder and is converted into an encoder output. 

The next code segment shows the standard encoder architecture code. Notice it is almost exactly like the GPT (decoder) code. The only difference is in the last layer.
	The pure Encoder does not need to predict words. Instead, it takes sentences and converts them to embeddings. In this case of size [B, 40, 512]. 


	
</p>




<center>
<div>
<textarea rows="24" cols="100">


class Encoder_Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   ## [65, 512]
        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]
        
        self.blocks = nn.Sequential(
                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]
        )
        
        self.ln_f    = nn.LayerNorm(  n_embd    )        
        
        
    def forward(self, idx, targets=None):
        B, T = idx.shape     ## (Batch, 40)
        ## ids and targets are both (B, T) tensors of integers
        
        tok_emb = self.token_embedding_table(idx)      
        pos_emb = self.pos_emb_table(torch.arange(T, device=device))  
        
        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]

        ## This is the architecture
        x = self.blocks(  x  )   ## (B, T, E)        
        x = self.ln_f(    x  )   ## (B, T, E)   ## [B, 40, 512]




  
</textarea>
  
</div>
    </center>


<p>
Notice that 6 identical encoder layers are created were the outputs of one become the inputs of the next layer. The dimensions of all inputs and output at this 
	stage are the same [B, 40, 512] where B is batch size.  The input is  a batch of "B" sentences, with 40 tokens per each sentence, and where 512 is 
	each id that has been embedded to a vector of size 512. Remember that the embedding vectors of size 512 are learned by the model so initially they are random data. 

We use a padding mask to ignore tokens with 0 value (i.e. padding).


BERTs are based on the encoder part of the original Transformer,  and are encoder only Transformers.

	
</p>
	
<h1>

	BERT
</h1>


<p>

BERT is an example of an encoder-only Transformer. In contrast to pure Encoders that take a sentence and only produce an embedding (e.g. [B, 40, 512]), BERT 
	models will add neural network layers (called heads) after the embedding (e.g. [B, 40, 512]) to convert those embeddings into words or sentences. 

BERT stands for Bidirectional Encoder Representations from Transformers. They were trained using 2 approaches. 

	
</p>

	

   <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/BERT_final_masking.png" height="900" width="auto">
      </div>

    </center>
  

<p>
The approaches are:
	
</p>


<ul>
<li>Masked language task training</li>
<li>Two sentence task training</li>
	
</ul>


	<p>

The first task BERT was pre-trained on was the Masked Language modeling approach. This approach is summarized in the previous figure. Here the BERT model 
		receives the same sentence as input and output. Some tokens are masked and the model needs to learn to predict those masked tokens during the training process. 

The second task BERT was pre-trained on is a 2 sentence classification task. The figure below summarizes this approach. Basically, here, 2 sentences are given to the BERT
		model as input. The model trains to predict if these 2 input sentences are sequential whcih means they are also related, or if they are not sequential and, 
		therefore, unrelated. 

BERT was originally developed by Google and there have now been several other versions that were developed. They all follow the simliar naming convention of being
		called RoBERTa, AlBERT, DistilBERT, etc.


		
	</p>



  <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/BERT_final_masking.png" height="900" width="auto">
      </div>

    </center>
	<br/>
	<br/>

	<h1>
Encoder Decoder Transformers
		
	</h1>
<p>

In this section, I will discuss the first version of the Transformer first made popular in the paper “Attention Is All You Need” by Vaswani et al. 
	They were first used for language translation. The Encoder Decoder with Multi Head Attention Transformer is a very deep network.  
The architecture has an encoder followed by a decoder. 

The decoder layer has 2 inputs. One input is the encoder output. The second input to the decoder varies based on whether you are training or predicting. 
	If you are training, the input to the decoder is the sentence in the other language. For instance, the Spanish sentence. In the decoder, when training the 
	Transformer, a mask is needed to prevent the model from seeing all the words it is trying to predict. This is called a look ahead mask. 

If you are testing, the input to the decoder is just the previous words before the word you are trying to predict. You start with a start of sentence token (e.g. <sos>)
	and predict iteratively. The predicted word is then added to the previous tokens and the process is repeated. 

In the Encoder Decoder Transformer (for the language translation problem),  the decoder layer has 2 inputs. One input is the encoder output. The second input to the 
	decoder varies based on whether you are training or predicting. If you are training, the input to the decoder is the sentence in the other language. For instance, 
	the Portuguese or Spanish sentence. When training, a mask is needed here to prevent the model from seeing all the words it is trying to predict. This is called a 
	look ahead mask. 


If you are testing, the input is just the previous words before the word you are trying to predict. You start with a start of sentence token (e.g. <sos>) and predict. 
	The predicted word is then added to the previous tokens and the process is repeated. The decoder consists of 6 decoder layers, followed by a linear layer. 
	Each decoder layer has a decoder multi-head attention layer, followed by a decoder-encoder attention layer, and a fully connected layer. 
	The decoder architecture for the Encoder Decoder Transformer (for the language translation problem) can be seen in the following figure.

	
</p>

 <center>
      <div>
        <img src="https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/images/decoder_layer.300dpi_lang_model.jpg" height="700" width="auto">
      </div>

    </center>
  

<p>
The attention layers consist of 8 parallel attention sub layers that are later concatenated. The numbers 6 and 8 are a choice the architect makes.
The first code segment in this section describes the decoder’s overall architecture. The decoder has more inputs than the encoder. 

The decoder_layer is the busiest function of the Transformer. It is basically very similar to the encoder_layer except that it has 2 attention mechanisms instead of
	just one. The Multi-Head Attention is the first attention mechanism. For our reference language problem, The Portuguese sentence and corresponding padding mask 
	are the only inputs to this sub layer. 
The output of this attention mechanism plus the encoder output are the inputs to the second attention mechanism which is usually referred to as the Encoder-Decoder-Attention
	mechanism. The output of this second Attention mechanism is passed to a fully connected layer just like the one used in the encoder. The first Masked multi-head
	attention layer is done 8 times in parallel just like in the encoder and the results are concatenated. This concatenated result is added to the original after 
	mapping it through one more layer to calculate the residual. 

The final layer maps a tensor of size [N, 40, 512] to a tensor of size [N, 40, pt_vocab_size] where pt_vocab_size is the size of the Portuguese vocabulary. 



This is what allows us to select the predicted word. 


	
</p>
	





	<h1>

Data Wrangling from Scratch
		
	</h1>
<p>

PyTorch offers many new techniques for extracting and processing data sets. As I like building things from scratch, I will present my own approach to data wrangling for 
	Transformers. The approach is very standard and is similar to what you do in NLP for algorithms like word2vec, for instance.

For this example, I will use the implementation of a Transformer-based Translator using the English to Portuguese dataset. The code and data set are available 
	on the book GitHub. First, let us import the libraries:

	
</p>




<center>
<div>
<textarea rows="15" cols="100">


import sklearn
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from numpy import genfromtxt
from sklearn import datasets
from sklearn.model_selection import train_test_split 
import pandas as pd
import pickle
import collections

  
</textarea>
  
</div>
    </center>
  


<p>

I like working with python dictionaries so, for this example, I extracted the data set and created python dictionaries for training and testing. 
	I saved the dictionaries to Python pickle files for ease of use. The following code shows how to load the dictionaries. 


</p>




 <center>
<div>
<textarea rows="10" cols="100">


def load_dictionary(file_name):
    with open(file_name, 'rb') as handle:
        dict = pickle.loads( handle.read() ) 
    return dict

  
</textarea>
  
</div>
    </center>



<p>

	
After loading the data sets from file, you have to create the dictionary and reverse dictionary. You create 2 dictionaries per language (e.g. two for English 
	and two for Portuguese). These  are dictionaries of ids to tokens and vice-cersa. Notice that I set the vocabulary size to 12,000. You can play with this 
	value for optimal performance.


</p>  







      <center>
<div>
<textarea rows="20" cols="100">

## Includes <eos> and <sos> tokens

def build_dataset(words):
    START_TOKEN = "<sos>"
    END_TOKEN   = "<eos>"
    UNK_TOKEN   = "<unk>"
    count = collections.Counter(words).most_common(12000) 
    dictionary = dict()
    for word, _ in count:
        ##add + 1 so that 0 is not used as index to avoid padding conflict 
        dictionary[word] = len(dictionary) + 1         ## + 1
    size_vocab = len(dictionary)
    dictionary[START_TOKEN] = size_vocab 
    dictionary[END_TOKEN]   = size_vocab + 1 
    dictionary[UNK_TOKEN]   = size_vocab + 2

    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
    
    return dictionary, reverse_dictionary


</textarea>
  
</div>
    </center>
  

<p>
The following is an example function in case you want to process sentences with regular expressions or tokenize manually. 



	
</p>




      <center>
<div>
<textarea rows="20" cols="100">


def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    # creating a space between a word and the punctuation following it 
    # eg: "it is a cat." => "it is a cat ."
    sentence = re.sub(r"([?.!,])", r" \1 ", sentence)
    sentence = re.sub(r'[" "]+', " ", sentence)
    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",") 
    
    sentence = re.sub(r"[^a-zA-Z?.!,]+", " ", sentence)
    sentence = sentence.strip()
    return sentence

  
</textarea>
  
</div>
    </center>
  



<p>

For tokenization, I used the NLTK tokenizer. In the paper “Attention Is All You Need”, the authors used byte pair encoding. Byte pair encoding does not use
	full words as tokens. Instead, you do something like this: "walk" and "ing" for the word “walking”. Therefore, words are broken into smaller elements 
	(subwords). Byte-pair encoding is used to tokenize sentences in a language, which, like the WordPiece encoding, breaks words up into tokens that are
	slightly larger than single characters but less than entire words. 


  
	
</p>





      <center>
<div>
<textarea rows="15" cols="100">


def get_tokens(sentence_list): 
    tokens_list = []
    for sentence in sentence_list:
        tokens = word_tokenize(sentence) 
        for word in tokens:
            tokens_list.append(word) 
    tokens_list = np.array(tokens_list) 
    return tokens_list



  
</textarea>
  
</div>
    </center>


<p>

	Once you have the dictionaries and the tokens, you can proceed to convert words into ids with the encode function.


</p>






<center>
<div>
<textarea rows="15" cols="100">


def encode(sentence, dictionary): 
    ids_list = []
    tokens = word_tokenize(sentence) 
    for word in tokens:
        if word in dictionary.keys(): 
            ids_list.append( dictionary[word] )
    return ids_list

  
</textarea>
  
</div>
    </center>



<p>


Decoding is just the process in reverse. Here you convert ids back to tokens using the reverse dictionary for convenience and speed up.


  
	
</p>





  
<center>
<div>
<textarea rows="10" cols="100">

def decode(list_ids, reverse_dictionary): 
    words_list = []
    for id in list_ids:
    if id in reverse_dictionary.keys(): 
        words_list.append( reverse_dictionary[id] )
    return words_list

  
</textarea>
  
</div>
    </center>



<p>

The following function aligns the English and Portuguese sentence pairs and creates two lists.


	
</p>

<center>
<div>
<textarea rows="20" cols="100">


## this returns 2 lists of english and portuguese sentences 
## that are aligned by index

def get_en_and_pt_sentences(train_dict): 
    en_list, pt_list = [], []
    for key, val in train_dict.items():
        print(key)
        print(val)
        en_list.append( val['en']  )
        pt_list.append( val['pt']  )
    return en_list, pt_list


</textarea>
  
</div>
    </center>


  
<p>


The below line of code just loads the sentences data before processing from the pickle objects.

	
</p>




<center>
<div>
<textarea rows="5" cols="100">

## Read in the data of english and portuguese sentences

train_dict = load_dictionary("data/en_pt_train_dictionary.txt") 
validation = load_dictionary("data/en_pt_val_dictionary.txt"  )

</textarea>
  
</div>
</center>



<p>
The next function creates 2 lists of aligned English and Portuguese sentences.

	
</p>




    

<center>
<div>
<textarea rows="5" cols="100">

english_sentence_list, portuguese_sentence_list = get_en_and_pt_sentences(train_dict)

</textarea>
  
</div>
    </center>


<p>
The function get_tokens converts each sentence into a list of tokens. 

	
</p>


  

<center>
<div>
<textarea rows="10" cols="100">


print("creating the dictionaries takes a while ... ")

en_tokens = get_tokens(english_sentence_list   ) 
pt_tokens = get_tokens(portuguese_sentence_list)


</textarea>
  
</div>
</center>



<p>


After creating the dictionaries for each language, we calculate the vocabulary size for each language. 
	
</p>



  

<center>
<div>
<textarea rows="15" cols="120">


## when 2 languages, you have 2 separate tokenizers.

en_dictionary, en_reverse_dictionary = build_dataset(en_tokens) pt_dictionary, pt_reverse_dictionary = build_dataset(pt_tokens)

VOCAB_SIZE_EN = len(en_dictionary) 
VOCAB_SIZE_PT = len(pt_dictionary)

print("vocab size english ",    VOCAB_SIZE_EN) 
print("vocab size portuguese ", VOCAB_SIZE_PT)


</textarea>
  
</div>
    </center>



<p>

The following “for” loop brings all the previous functions together. It results in 2 lists of sentence ids, one for each language  (2 lists of Numpy objects). 
	Notice that, to each sentence list of ids, we add the start token id at the beginning and the end token id at the end. 

The final “if” statement is used to only include sentences shorter than 40 tokens (the max length I used). Sentences shorter than 40 will be padded but all 
	sentences will eventually be tensors of size 40 (n_tokens + padding).

	
</p>




   <center>
<div>
<textarea rows="25" cols="100">


english_sentence_ids_list = [] 
portuguese_sentence_ids_list = []

for i in range( len(english_sentence_list) ): 

    en_sentence = english_sentence_list[i] 
    pt_sentence = portuguese_sentence_list[i]
    
    en_sentence_ids = encode(en_sentence, en_dictionary) 
    pt_sentence_ids = encode(pt_sentence, pt_dictionary)
    
    en_sentence_ids = np.array(en_sentence_ids) 
    pt_sentence_ids = np.array(pt_sentence_ids)
    
    en_START_TOKEN_id = en_dictionary['<sos>'] 
    en_END_TOKEN_id   = en_dictionary['<eos>']
    
    pt_START_TOKEN_id = pt_dictionary['<sos>'] 
    pt_END_TOKEN_id   = pt_dictionary['<eos>']
    
    en_sentence_ids = np.concatenate(
        [ [en_START_TOKEN_id], en_sentence_ids, [en_END_TOKEN_id] ] )
        
    pt_sentence_ids = np.concatenate(
        [ [pt_START_TOKEN_id], pt_sentence_ids, [pt_END_TOKEN_id] ] )
        
    if len(en_sentence_ids) <= MAX_LENGTH and len( pt_sentence_ids) <= MAX_LENGTH: 
        english_sentence_ids_list.append(    en_sentence_ids) 
        portuguese_sentence_ids_list.append( pt_sentence_ids)


</textarea>
  
</div>
    </center>
  

<p>
Now we need to use a Torch padding function. 

  
	
</p>

 


   <center>
<div>
<textarea rows="14" cols="100">


en_MAX_LENGTH = MAX_LENGTH 
pt_MAX_LENGTH = MAX_LENGTH + 1

english_sentence_ids_list = torch.preprocessing.sequence.pad_sequences( english_sentence_ids_list, maxlen=en_MAX_LENGTH, padding='post')

portuguese_sentence_ids_list = torch.preprocessing.sequence.pad_sequences( portuguese_sentence_ids_list, maxlen=pt_MAX_LENGTH, padding='post')


</textarea>
  
</div>
</center>





<p>


	If you would like to view the data, you can do so with the following code.


</p>



 
<center>
<div>
<textarea rows="10" cols="80">


for i in range( len(english_sentence_ids_list) ):   
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@") 
    print(english_sentence_ids_list[i]) 
    print(portuguese_sentence_ids_list[i])
    ## input()


</textarea>
  
</div>
    </center>




<p>
After padding, the data will look like this:

</p>







<center>
<div>
<textarea rows="20" cols="100">

en
    
[12110   203     4  3947    29     2   168     2     4    27    68  
  4333     8  3622  2943  1012     1 12111     0     0     0     0    
     0     0     0     0     0     0     0     0     0     0     0     
     0     0     0     0     0     0     0 ]

pt
         
[12210    13     4  3947    29     2     5    32    36    16  1145     
     4    58    34  7905    58    25    28   354  2482     3    17    
    27    28  4395     9  2886     7 12211     0     0     0     0     
     0     0     0     0     0     0     0 ]


</textarea>
  
</div>
    </center>





<p>

And without padding, the data will look like this: 

   
	
</p>





     <center>
<div>
<textarea rows="20" cols="100">


en
    
[12110    13     4  3947    29     2     5    32    36    16  
  1145     4    58    34  7905    58    25    28   354  2482     
     3    17    27    28  4395     9  2886     7 12111    ]
         
pt
         
[12210    62   585   132   202  4395 11969     3    43    18    
    27   107  7042    15    10   814 11717     4  4053    89  
  2960     2   157   119     1 12211     ]


</textarea>
  
</div>
    </center>

         


<h1>Summary</h1>
                
<p>

  In this chapter, I have introduced the topic of Transformers. I discussed the main ideas and code for the Encoder Decoder with Multi-Head Attention 
  Transformer first introduced by Vaswani et al. (2017), ideas of BERTs, and the GPT.

</p>



</div>  <!-- for the fixed nav bar -->

    
  </body>
</html>
